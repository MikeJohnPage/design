[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy design principles",
    "section": "",
    "text": "Welcome\nThe goal of this book is to help you write better R code. It has four main components:\n\nIdentifying design challenges that often lead to suboptimal outcomes.\nIntroducing useful patterns that help solve common problems.\nDefining key principles that help you balance conflicting patterns.\nDiscussing case studies that help you see how all the pieces fit together with real code.\n\nWhile I’ve called these principles “tidy” and they’re used extensively by the tidyverse team to promote consistency across our packages, they’re not exclusive to the tidyverse. Think tidy in the sense of tidy data (broadly useful regardless of what tool you’re using) not tidyverse (a collection of functions designed with a singular point of view in order to facilitate learning and use).\nThis book will be under heavy development for quite some time; currently we are loosely aiming for completion in 2025. You’ll find many chapters contain disjointed text that mostly serve as placeholders for the authors, and I do not recommend attempting to systematically read the book at this time. If you’d like to follow along with my journey writing this book, and learn which chapters are ready to read, please sign up for my tid design substack mailing list."
  },
  {
    "objectID": "unifying.html#human-centered",
    "href": "unifying.html#human-centered",
    "title": "Unifying principles",
    "section": "Human centered",
    "text": "Human centered\n\nPrograms must be written for people to read, and only incidentally for machines to execute.\n— Hal Abelson\n\nProgramming is a task performed by humans. To create effective programming tools we must explicitly recognise and acknowledge the role played by cognitive psychology. This is particularly important for R, because it’s a language that’s used primarily by non-programmers, and we want to make it as easy as possible for first-time and end-user programmers to learn the tidyverse.\nA particularly useful tool from cognitive psychology is “cognitive load theory”1: we have a limited working memory, and anything we can do to reduce extraneous cognitive load helps the learner and user of the tidyverse. This motivates the next two principles:\n\nBy being consistent you only need to learn and internalise one expression of an idea, and then you can apply that many times.\nBy being composable you can break down complex problems into bite sized pieces that you can easily hold in your head.\n\nIdea of “chunking” is important. Some setup cost to learn a new chunk, but once you’ve internalised it, it only takes up one spot in your working memory. In some sense the goal of the tidyverse is to discover the minimal set of chunks needed to do data science and have some sense of the priority of the remainder.\nOther useful ideas come from design. One particularly powerful idea is that of “affordance”: the exterior of a tool should suggest how to use it. We want to avoid “Norman doors” where the exterior clues and cues point you in the wrong direction.\nThis principle is deeply connected to our beliefs about performance. Most importantly performance of code depends not only on how long it takes to run, but also how long it takes to write and read. Human brains are typically slower than computers, so this means we spend a lot of time thinking about how to create intuitve interfaces, focussing on writing and reading speed. Intuitive interfaces sometimes are at odds with running speed, because writing the fastest code for a problem often requires designing the interface for performance rather than usability. Generally, we optimise first for humans, then use profiling to discover bottlenecks that cause friction in data analysis. Once we have identified an important bottleneck, then performance becomes a priority and we rewrite the existing code. Generally, we’ll attempt to preserve the existing interface, only changing it when the performance implications are significant."
  },
  {
    "objectID": "unifying.html#consistent",
    "href": "unifying.html#consistent",
    "title": "Unifying principles",
    "section": "Consistent",
    "text": "Consistent\n\nA system should be built with a minimum set of unchangeable parts; those parts should be as general as possible; and all parts of the system should be held in a uniform framework.\n— Daniel H. H. Ingalls\n\nThe most important API principle of the tidyverse is to be consistent. We want to find the smallest possible set of key ideas and use them again and again. This is important because it makes the tidyverse easier to learn and remember.\n(Another framing of this principle is Less Volume, More Creativity, which comes from Mike McCarthy, the head coach of the Green Bay Packers, and popularised in Statistics Education by Randall Pruim)\nThis is related to one of my favourite saying from the Python community:\n\nThere should be one—and preferably only one—obvious way to do it.\n— Zen of Python\n\nThe tidyverse aspires to put this philosophy into practice. However, because the tidyverse is embedded within the larger R ecosystem, applying this principle never needs to be 100% comprehensive. If you can’t solve a problem from within the tidyverse, you can always step outside and do so with base R or another package. This also means that we don’t have to rush to cover every possible use case; we can take our time to develop the best new solutions.\nThe principle of consistency reveals itself in two primary ways: in function APIs and in data structures. The API of a function defines its external interface (independent of its internal implementation). Having consistent APIs means that each time you learn a function, learning the next function is a little easier; once you’ve mastered one package, mastering the next is easier.\nThere are two ways that we make functions consistent that are so important that they’re explicitly pull out as high-level principles below:\n\nFunctions should be composable: each individual function should tackle one well contained problem, and you solve complex real-world problems by composing many individual functions.\nOverall, the API should feel “functional”, which is a technical term for the programming paradigm favoured by the tidyverse\n\nBut consistency also applies to data structures: we want to ensure we use the same data structures again and again and again. Principally, we expect data to be stored in tidy data frames or tibbles. This means that tools for converting other formats can be centralised in one place, and that packages development is simplified by assuming that data is already in a standard format.\nValuing consistency is a trade-off, and we explicitly value it over performance. There are cases where a different data structure or a different interface might make a solution simpler to express or much faster. However, one-off solutions create a much higher cognitive load."
  },
  {
    "objectID": "unifying.html#composable",
    "href": "unifying.html#composable",
    "title": "Unifying principles",
    "section": "Composable",
    "text": "Composable\n\nNo matter how complex and polished the individual operations are, it is often the quality of the glue that most directly determines the power of the system.\n— Hal Abelson\n\nA powerful strategy for solving complex problems is to combine many simple pieces. Each piece should be easily understood in isolation, and have a standard way of combining with other pieces.\nWithin the tidyverse, we prefer to compose functions using a single tool: the pipe, %&gt;%. There are two notable exceptions to this principle: ggplot2 composes graphical elements with +, and httr composes requests primarily through .... These are not bad techniques in isolation, and they are well suited to the domains in which they are used, but the disadvantages of inconsistency outweigh any local advantages.\nFor smaller domains, this means carefully designing functions so that the inputs and outputs align (e.g. the output from stringr::str_locate() can easily be fed into str_sub()). For middling domains, this means drawing many feature matrices and ensuring that they are dense (e.g. consider the map family in purrr). For larger domains, this means carefully thinking about algebras and grammars, identifying the atoms of a problem and the ways in which they might be composed to solve bigger problems.\nWe decompose large problems into smaller, more tractable ones by creating and combining functions that transform data rather than by creating objects whose state changes over time.\nOther techniques that tend to facilitate composability:\n\nFunctions are data: this leads some of the most impactful techniques for functional programming, which allow you to reduce code duplication.\nImmutable objects. Enforces independence between components.\nPartition side-effects.\nType-stable."
  },
  {
    "objectID": "unifying.html#inclusive",
    "href": "unifying.html#inclusive",
    "title": "Unifying principles",
    "section": "Inclusive",
    "text": "Inclusive\nWe value not just the interface between the human and the computer, but also the interface between humans. We want the tidyverse to be a diverse, inclusive, and welcoming community.\n\nWe develop educational materials that are accessible to people with many different skill levels.\nWe prefer explicit codes of conduct.\nWe create safe and friendly communities. We believe that kindness should be a core value of communities.\nWe think about how we can help others who are not like us (they may be visually impaired or may not speak English).\n\nWe also appreciate the paradox of tolerance: the only people that we do not welcome are the intolerant."
  },
  {
    "objectID": "unifying.html#footnotes",
    "href": "unifying.html#footnotes",
    "title": "Unifying principles",
    "section": "",
    "text": "A good practical introduction is Cognitive load theory in practice (PDF).↩︎"
  },
  {
    "objectID": "names.html#coverage-in-tidyverse-style-guide",
    "href": "names.html#coverage-in-tidyverse-style-guide",
    "title": "Names attribute",
    "section": "Coverage in tidyverse style guide",
    "text": "Coverage in tidyverse style guide\nExisting name-related topics in http://style.tidyverse.org\n\nFile names\nObject names\nArgument names\nFunction names"
  },
  {
    "objectID": "names.html#the-names-attribute-of-an-object",
    "href": "names.html#the-names-attribute-of-an-object",
    "title": "Names attribute",
    "section": "The names attribute of an object",
    "text": "The names attribute of an object\nHere we address how to manage the names attribute of an object. Our initial thinking was motivated by how to handle the column or variable names of a tibble, but is evolving into a name-handling strategy for vectors, in general.\nThe name repair described below is exposed to users via the .name_repair argument of tibble::tibble(), tibble::as_tibble(), readxl::read_excel(), and, eventually other packages in the tidyverse. This work was initiated in the tibble package, but is migrating to the vctrs package. Name repair was first introduced in tibble v2.0.0 and this write-up is being rendered with tibble v3.2.1 and vctrs v0.6.4.\nThese are the kind of names we’re talking about:\n\n## variable names\nnames(iris)\n#&gt; [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"\n\nnames(ChickWeight)\n#&gt; [1] \"weight\" \"Time\"   \"Chick\"  \"Diet\"\n\n## names along a vector\nnames(euro)\n#&gt;  [1] \"ATS\" \"BEF\" \"DEM\" \"ESP\" \"FIM\" \"FRF\" \"IEP\" \"ITL\" \"LUF\" \"NLG\" \"PTE\""
  },
  {
    "objectID": "names.html#minimal-unique-universal",
    "href": "names.html#minimal-unique-universal",
    "title": "Names attribute",
    "section": "Minimal, unique, universal",
    "text": "Minimal, unique, universal\nWe identify three nested levels of naminess that are practically useful:\n\nMinimal: The names attribute is not NULL. The name of an unnamed element is \"\" (the empty string) and never NA.\nUnique: No element of names appears more than once. A couple specific names are also forbidden in unique names, such as \"\" (the empty string).\n\nAll columns can be accessed by name via df[[\"name\"]] and, more generally, by quoting with backticks: df$`name`, subset(df, select = `name`), and dplyr::select(df, `name`).\n\n\nUniversal: The names are unique and syntactic.\n\nNames work everywhere, without quoting: df$name and lm(name1 ~ name2, data = df) and dplyr::select(df, name) all work.\n\n\n\nBelow we give more details and describe implementation."
  },
  {
    "objectID": "names.html#minimal-names",
    "href": "names.html#minimal-names",
    "title": "Names attribute",
    "section": "Minimal names",
    "text": "Minimal names\nMinimal names exist. The names attribute is not NULL. The name of an unnamed element is \"\" (the empty string) and never NA.\nConsider an unnamed vector, i.e. it has names attribute of NULL.\n\nx &lt;- letters[1:3]\nnames(x)\n#&gt; NULL\n\nThis means that the names of x are sometimes a character vector the same length of x, and sometimes NULL. rlang papers of this problem by providing names2() which always returns a character vector:\n\nrlang::names2(x)\n#&gt; [1] \"\" \"\" \"\"\n\nAnd you can also use this to ensure a vector has minimal names:\n\nnames(x) &lt;- rlang::names2(x)\nnames(x)\n#&gt; [1] \"\" \"\" \"\"\n\nMinimal names appear to be a useful baseline requirement, if the names attribute of an object is going to be actively managed. Why? General name handling and repair can be implemented more simply if the baseline strategy guarantees that names(x) returns a character vector of the correct length with no NAs.\nThis is also a reasonable interpretation of base R’s intent for named vectors, based on the docs for names(), although base R’s implementation/enforcement of this is uneven. From ?names:\n\nThe name \"\" is special: it is used to indicate that there is no name associated with an element of a (atomic or generic) vector. Subscripting by \"\" will match nothing (not even elements which have no name).\nA name can be character NA, but such a name will never be matched and is likely to lead to confusion.\n\ntbl_df objects created by tibble::tibble() and tibble::as_tibble() have variable names that are minimal, at the very least."
  },
  {
    "objectID": "names.html#unique-names",
    "href": "names.html#unique-names",
    "title": "Names attribute",
    "section": "Unique names",
    "text": "Unique names\nUnique names meet the requirements for minimal and have no duplicates. In the tidyverse, we go further and repair a few specific names: \"\" (the empty string), ... (R’s ellipsis or “dots” construct), and ..j where j is a number. They are basically all treated like \"\", which is always repaired.\nExample of unique-ified names:\n\n## original unique-ified\n##       \"\"         ...1\n##        x        x...2\n##       \"\"         ...3\n##      ...         ...4\n##        y            y\n##        x        x...6\n\nThis augmented definition of unique has a specific motivation: it ensures that each element can be identified by name, at least when protected by backtick quotes. Literally, all of these work:\n\ndf[[\"name\"]]\ndf$`name`\nwith(df, `name`)\nsubset(df, select = `name`)\ndplyr::select(df, `name`)\n\nThis has practical significance for variable names inside a data frame, because so many workflows rely on indexing by name. Note that uniqueness refers implicitly to a vector of names.\nLet’s explore a few edge cases: A single dot followed by a number, .j, does not need repair.\n\ndf &lt;- tibble(`.1` = \"ok\")\ndf$`.1`\n#&gt; [1] \"ok\"\nsubset(df, select = `.1`)\n#&gt; # A tibble: 1 × 1\n#&gt;   `.1` \n#&gt;   &lt;chr&gt;\n#&gt; 1 ok\ndplyr::select(df, `.1`)\n#&gt; # A tibble: 1 × 1\n#&gt;   `.1` \n#&gt;   &lt;chr&gt;\n#&gt; 1 ok\n\nTwo dots followed by a number, ..j, does need repair. The same goes for three dots, ..., the ellipsis or “dots” construct. These can’t function as names, even if quoted with backticks, so they have to be repaired.\n\ndf &lt;- tibble(`..1` = \"not ok\")\n#&gt; Error in `tibble()`:\n#&gt; ! Column 1 must not have names of the form ... or ..j.\n#&gt; Use `.name_repair` to specify repair.\n#&gt; Caused by error in `repaired_names()`:\n#&gt; ! Names can't be of the form `...` or `..j`.\n#&gt; ✖ These names are invalid:\n#&gt;   * \"..1\" at location 1.\nwith(df, `..1`)\n#&gt; Error in eval(substitute(expr), data, enclos = parent.frame()): ..1 used in an incorrect context, no ... to look in\ndplyr::select(df, `..1`)\n#&gt; Error in dot_call(capture_dots, frame_env = frame_env, named = named, : '...' used in an incorrect context\n\ndf &lt;- tibble(`...` = \"not ok\")\n#&gt; Error in `tibble()`:\n#&gt; ! Column 1 must not have names of the form ... or ..j.\n#&gt; Use `.name_repair` to specify repair.\n#&gt; Caused by error in `repaired_names()`:\n#&gt; ! Names can't be of the form `...` or `..j`.\n#&gt; ✖ These names are invalid:\n#&gt;   * \"...\" at location 1.\nsubset(df, select = `...`)\n#&gt; Error in eval(expr, envir, enclos): '...' used in an incorrect context\ndplyr::select(df, `...`)\n#&gt; Error in eval(expr, envir, enclos): '...' used in an incorrect context\n\nBoth are repaired as if they were \"\".\nMaking names unique\nThere are many ways to make names unique. We append a suffix of the form ...j to any name that is a duplicate or \"\" or ..., where j is the position. Why?\n\nAn absolute position j is more helpful than numbering within the elements that share a name. Context: troubleshooting data import with lots of columns and dysfunctional names.\nWe hypothesize that it’s better have a “level playing field” when repairing names, i.e. if foo appears twice, both instances get repaired, not just the second occurrence.\n\nThe unique level of naminess is regarded as normative for a tibble and a user must expressly request a tibble with names that violate this (but that is possible).\nBase R’s function for this is make.unique(). We revisit the example above, comparing the tidyverse strategy for making names unique vs. what make.unique() does.\n\n## Original Unique names     Result of\n##    names  (tidyverse) make.unique()\n##       \"\"         ...1            \"\"\n##        x        x...2             x\n##       \"\"         ...3            .1\n##      ...         ...4           ...\n##        y            y             y\n##        x        x...6           x.1\n\nRoundtrips\nWhen unique-ifying names, we assume that the input names have been repaired by the same strategy, i.e. that we are consuming dogfood. Therefore, pre-existing suffixes of the form ...j are stripped, prior to (re-)constructing the suffixes. If this interacts poorly with your names, you need to take control of name repair.\nExample of re-unique-ified names:\n\n##  original unique-ified\n##      ...5         ...1\n##         x        x...2\n##     x...3        x...3\n##        \"\"         ...4\n## x...1...5        x...5\n\nJB: it is conceivable that this should be under the control of an argument, e.g. dogfood = TRUE, in the (currently unexported) function that does this\nWhen is minimal better than unique?\nWhy would you ever want to import a tibble and enforce only minimal names, instead of unique? Sometimes the first row of a data source – allegedly variable names – actually contains data and the resulting tibble will be reshaped with, e.g., tidyr::gather(). In this case, it is better to not munge the names at import. This is a common special case of the “data stored in names” phenomenon.\nIn general, you may want to tolerate minimal names when the dysfunctional names are just an awkward phase that an object is passing through and a more definitive solution is applied downstream.\nUgly, with a purpose\nYou might say that names like x...5 are ugly and you would be right. We’re calling this a feature, not a bug! Names that have been automatically unique-ified by the tidyverse should catch the eye and give the user strong encouragement to take charge of the situation.\nWhy so many dots?\nThe suffix of ...j, with 3 leading dots, is the result of jointly satisfying multiple requirements. It is important to anticipate a missing name, where the suffix becomes the entire name. We have elected to make the suffix a syntactic name (more below), because non-syntactic names are a frequent cause of unexpected friction for users. This means the suffix can’t be j, .j, or ..j, because all are non-syntactic. It must be ...j.\nWhy dot(s) in the first place?\nThe underscore _ was also considered when choosing the suffix strategy, but was rejected. Why? Because syntactic names can’t start with an underscore and we want the suffix itself to be syntactic. Also, the dot . is already used by base R’s make.names() to replace invalid characters. It seems simpler and, therefore, better to use the same character, in the same way, as much as possible in name repair. We use the dot ., we put it at the front, as many times as necessary."
  },
  {
    "objectID": "names.html#universal-names",
    "href": "names.html#universal-names",
    "title": "Names attribute",
    "section": "Universal names",
    "text": "Universal names\nUniversal names are unique, in the sense described above, and syntactic, in the normal R sense. Universal names are appealing because they play nicely with base R and tidyverse functions that accept unquoted variable names.\nSyntactic names\nA syntactic name in R:\n\nConsists of letters, numbers, and the dot . or underscore _ characters.\nStarts with a letter or starts with a dot . followed by anything but a number.\nIs not a reserved word, such as if or function or TRUE.\nIs not ..., R’s special ellipsis or “dots” construct.\nIs not of the form ..j, where j is a number.\n\nSee R’s documentation for Reserved words and Quotes, specifically the section on names and identifiers.\nA syntactic name can be used “as is” in code. For example, it does not require quoting in order to work with non-standard evaluation, such as list indexing via $, in a formula, or in packages like dplyr and ggplot2.\n\n## a syntactic name doesn't require quoting\nx &lt;- tibble::tibble(.else = \"else?!\")\nx$.else\n#&gt; [1] \"else?!\"\ndplyr::select(x, .else)\n#&gt; # A tibble: 1 × 1\n#&gt;   .else \n#&gt;   &lt;chr&gt; \n#&gt; 1 else?!\n\n\n## use a non-syntactic name\nx &lt;- tibble::tibble(`else` = \"else?!\")\n\n## this code does not parse\n# x$else\n# dplyr::select(x, else)\n\n## a non-syntacitic name requires quoting\nx$`else`\n#&gt; [1] \"else?!\"\ndplyr::select(x, `else`)\n#&gt; # A tibble: 1 × 1\n#&gt;   `else`\n#&gt;   &lt;chr&gt; \n#&gt; 1 else?!\n\nNote that being syntactic is a property of an individual name.\nMaking an individual name syntactic\nThere are many ways to fix a non-syntactic name. Here’s how our logic compares to base::make.names() for a single name:\n\nSame: Definition of what is syntactically valid.\n\nClaim: If syn_name is a name that we have made syntactic, then syn_name == make.names(syn_name). If you find a counterexample, tell us!\n\n\nSame: An invalid character is replaced with a dot ..\nDifferent: We always fix a name by prepending a dot .. base::make.names() sometimes prefixes with X and at other times appends a dot ..\n\nThis means we turn ... into .... and ..j into ...j, where j is a number. base::make.names() does not modify ... or ..j, which could be regarded as a bug (?).\n\n\nDifferent: We treat NA and \"\" the same: both become .. This is because we first make names minimal. base::make.names() turns NA into \"NA.\" and \"\" into \"X\".\n\nExamples of the tidyverse approach to making individual names syntactic versus base::make.names():\n\n## Original Syntactic name    Result of\n##     name    (tidyverse) make.names()\n##       \"\"              .            X\n##       NA              .          NA.\n##      (y)            .y.         X.y.\n##       _z            ._z          X_z\n##     .2fa          ..2fa        X.2fa\n##    FALSE         .FALSE       FALSE.\n##      ...           ....          ...\n##      ..3           ...3          ..3\n\n\nCurrently implemented in the unexported function tibble:::make_syntactic().\nWhy universal?\nNow we can state the motivation for universal names, which have the group-wise property of being unique and the element-wise property of being syntactic.\nIn practice, if you want syntactic names, you probably also want them to be unique. You need both in order to refer to individual elements easily, without ambiguity and without quoting.\nUniversal names can be requested in the tidyverse via .name_repair = \"universal\", in functions that expose name repair.\nMaking names universal\nUniversal names are implemented as a variation on unique names. Basically, suffixes are stripped and ... is replaced with \"\". These draft names are transformed with tibble:::make_syntactic() (this step is omitted for unique names). Then ...j suffixes are appended as necessary.\nNote that suffix stripping and the substitution of \"\" for ... happens before the draft names are made syntactic. So, although tibble:::make_syntactic turns ... into ...., universal or unique name repair will turn ... into something of the form ...j."
  },
  {
    "objectID": "names.html#messaging-user-about-name-repair",
    "href": "names.html#messaging-user-about-name-repair",
    "title": "Names attribute",
    "section": "Messaging user about name repair",
    "text": "Messaging user about name repair\nName repair should be communicated to the user. Here’s how tibble messages:\n\nx &lt;- tibble::tibble(\n  x = 1, x = 2, `a1:` = 3, `_x_y}` = 4,\n  .name_repair = \"universal\"\n)\n#&gt; New names:\n#&gt; • `x` -&gt; `x...1`\n#&gt; • `x` -&gt; `x...2`\n#&gt; • `a1:` -&gt; `a1.`\n#&gt; • `_x_y}` -&gt; `._x_y.`"
  },
  {
    "objectID": "call-data-details.html#whats-the-pattern",
    "href": "call-data-details.html#whats-the-pattern",
    "title": "Name all but the most important arguments",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nWhen calling a function, you should name all but the most important arguments. For example:\n\ny &lt;- c(1:10, NA)\nmean(y, na.rm = TRUE)\n#&gt; [1] 5.5\n\nNever use partial matching, like below. Partial matching was useful in the early days of R because when you were doing a quick and dirty interactive analysis you could save a little time by shortening argument names. However, today, most R editing environments support autocomplete so partial matching only saves you a single keystroke, and it makes code substantially harder to read.\n\nmean(y, n = TRUE)\n#&gt; [1] 5.5\n\nAvoid relying on position matching with empty arguments:\n\nmean(y, , TRUE)\n#&gt; [1] 5.5\n\nAnd don’t name arguments that can you expect users to be familiar with:\n\nmean(x = y)\n#&gt; [1] NA\n\nYou can make R give you are warning that you’re using a partially named argument with a special option. Call usethis::use_partial_warnings() to make this the default for all R sessions.\n\noptions(warnPartialMatchArgs = TRUE)\nmean(x = 1:10, n = FALSE)\n#&gt; Warning in mean.default(x = 1:10, n = FALSE): partial argument match of 'n' to\n#&gt; 'na.rm'\n#&gt; [1] 5.5"
  },
  {
    "objectID": "call-data-details.html#why-is-this-useful",
    "href": "call-data-details.html#why-is-this-useful",
    "title": "Name all but the most important arguments",
    "section": "Why is this useful?",
    "text": "Why is this useful?\nI think it’s reasonable to assume that the reader knows what a function does then they know what the one or two most important arguments are, and repeating their names just takes up space without aiding communication. For example, it’s reasonable to assume that people can remember that the first argument to log() is x and the first two arguments to dplyr::left_join() are x and y.\nHowever, I don’t think that most people will remember more than the one or two most important arguments, so you should name the rest. For example, I don’t think that most people know that the second argument to mean() is trim or that the second argument to median() is na.rm even though I expect most people to know what the first arguments are. Spelling out the names makes it easier to understand when others (including future you) are reading the code."
  },
  {
    "objectID": "call-data-details.html#what-are-the-exceptions",
    "href": "call-data-details.html#what-are-the-exceptions",
    "title": "Name all but the most important arguments",
    "section": "What are the exceptions?",
    "text": "What are the exceptions?\nThere are two main exceptions to this principle: when teaching functions and when one argument is particularly long.\nWhen teaching a function for the first time, you can’t expect people to know what the arguments are, so it make sense to supply all names to help people understand exactly what’s going on. For example, in R for Data Science when we introduce ggplot2 we write code like:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point()\n\nAt the end of the chapter, we assume that the reader is familiar with the basic structure and so the rest of the book uses the style recommended here:\n\nggplot(mpg, aes(`displ, hwy)) + \n  geom_point()\n\nThere are also the occasional case when the first argument might be quite long, and there’s a couple of short options that you also want to set. If the long argument comes first, you may have to re-interpret what the function is doing when you finally hit the options. I think this comes up most often when an argument usually receives code inside of {} but it can crop up when manually generating data too.\n\nwriteLines(con = \"test.txt\", c(\n  \"line1\",\n  \"line2\",\n  \"line3\"\n))\n\nexpect_snapshot(error = TRUE, {\n  line1\n  line2\n  line3\n})"
  },
  {
    "objectID": "function-names.html#nouns-vs-verbs",
    "href": "function-names.html#nouns-vs-verbs",
    "title": "Function names",
    "section": "Nouns vs verbs",
    "text": "Nouns vs verbs\nIn general, prefer verbs. Use imperative mood: mutate() not mutated(), mutates(), or mutating(); do() not did(), does(), doing(), hide() not hid(), hides(), or hiding().\nException: noun-y interfaces where you’re building up a complex object like ggplot2 or recipes (verb-y interface in ggvis was a mistake).\nNouns should be singular (geom_point() not geom_points()), simply because the plurisation rules in English are complex."
  },
  {
    "objectID": "function-names.html#function-families",
    "href": "function-names.html#function-families",
    "title": "Function names",
    "section": "Function families",
    "text": "Function families\nUse prefixes to group functions together based on common input or common purpose. Prefixes are better than suffixes because of auto-complete. Examples: ggplot2, purrr. Counter example: shiny.\nNot sure about common prefixes for a package. Works well for stringr (esp. with stringi), forcats, xml2, and rvest. But there’s only a limited number of short prefixes and I think it would break down if every package did it.\nUse suffixes for variations on a theme (e.g. map_int(), map_lgl(), map_dbl(); str_locate(), str_locate_all().)\nStrive for thematic unity in related functions. Can you make related fuctions rhyme? Or have the same number of letters? Or similar background (i.e. all Germanic origins vs. French)."
  },
  {
    "objectID": "function-names.html#length",
    "href": "function-names.html#length",
    "title": "Function names",
    "section": "Length",
    "text": "Length\nErr on the side of too long rather than too short (reading is generally more important than writing). Autocomplete will mostly take care of the nuisance and you can always shorten later if you come up with a better name. (But hard to make long later, and you may take up a good word that is a lot of work to reclaim later).\nLength of name should be inversely proportional to frequency of usage. Reserve very short words for functions that are likely to be used very frequently."
  },
  {
    "objectID": "function-names.html#conflicts",
    "href": "function-names.html#conflicts",
    "title": "Function names",
    "section": "Conflicts",
    "text": "Conflicts\nYou can’t expect to avoid conflicts with every existing CRAN package, but you should strive to avoid conflicts with “nearby” packages (i.e. packages that are commonly used with your package)."
  },
  {
    "objectID": "function-names.html#techniques",
    "href": "function-names.html#techniques",
    "title": "Function names",
    "section": "Techniques",
    "text": "Techniques\n\nThesaurus\nList of common verbs\nRhyming dictionary"
  },
  {
    "objectID": "function-names.html#other-good-advice",
    "href": "function-names.html#other-good-advice",
    "title": "Function names",
    "section": "Other good advice",
    "text": "Other good advice\n\nI Shall Call It.. SomethingManager\nThe Poetry of Function Naming"
  },
  {
    "objectID": "inputs-explicit.html#whats-the-problem",
    "href": "inputs-explicit.html#whats-the-problem",
    "title": "Make inputs explicit",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nA function is easier to understand if its output depends only on its inputs (i.e. its arguments). If a function returns different results with the same inputs, then some inputs must be implicit, typically because the function relies on an option or some locale setting. Implicit inputs are not always bad, as some functions like Sys.time(), read.csv(), and the random number generators, fundamentally depend on them. But they should be used as sparingly as possible, and never when not related to the core purpose of the function.\nExplicit arguments make code easier to understand because you can see what will affect the outputs just by reading the code; you don’t need to run it. Implicit arguments can lead to code that returns different results on different computers, and the differences are usually hard to track down."
  },
  {
    "objectID": "inputs-explicit.html#what-are-some-examples",
    "href": "inputs-explicit.html#what-are-some-examples",
    "title": "Make inputs explicit",
    "section": "What are some examples?",
    "text": "What are some examples?\nOne common source of hidden arguments is the use of global options:\n\nHistorically, the worst offender was the stringsAsFactors option which changed how a number of functions1 treated character vectors. This option was part of a multi-year procedure to move R away toward character vectors and away from vectors. You can learn more in stringsAsFactors: An unauthorized biography by Roger Peng and stringsAsFactors = &lt;sigh&gt; by Thomas Lumley.\nlm()’s handling of missing values depends on the global option of na.action. The default is na.omit which drops the missing values prior to fitting the model (which is inconvenient because then the results of predict() don’t line up with the input data.\n\nAnother common source of subtle bugs is relying on the system locale, i.e. the country and language specific settings controlled by your operating system. Relying on the system locale is always done with the best of intentions (you want your code to respect the user’s preferences) but can lead to subtle differences when the same code is run by different people. Here are a few examples:\n\nstrptime() relies on the names of weekdays and months in the current locale. That means strptime(\"1 Jan 2020\", \"%d %b %Y\") will work on computers with an English locale, and fail elsewhere.\n\nas.POSIXct() depends on the current timezone. The following code returns different underlying times when run on different computers:\n\nas.POSIXct(\"2020-01-01 09:00\")\n#&gt; [1] \"2020-01-01 09:00:00 UTC\"\n\n\ntoupper() and tolower() depend on the current locale. It is fairly uncommon for this to cause problems because most languages either use their own character set, or use the same rules for capitalisation as English. However, this behaviour did cause a bug in ggplot2 because internally it takes geom = \"identity\" and turns it into GeomIdentity to find the object that actually does computation. In Turkish, however, the upper case version of i is İ, and Geomİdentity does not exist. This meant that for some time ggplot2 did not work on Turkish computers.\nsort() and order() rely on the lexicographic order (i.e. how different alphabets sort their letters) defined by the current locale. lm() automatically converts character vectors to factors with factor(), which uses order(), which means that it’s possible for the coefficients to vary2 if your code is run in a different country!"
  },
  {
    "objectID": "inputs-explicit.html#how-can-i-remediate-the-problem",
    "href": "inputs-explicit.html#how-can-i-remediate-the-problem",
    "title": "Make inputs explicit",
    "section": "How can I remediate the problem?",
    "text": "How can I remediate the problem?\nAt some level, implicit inputs are easy to avoid when creating new functions: just don’t use the locale or global options! But it’s easy for such problems to creep in indirectly, when you call a function not knowing that it has hidden inputs. The best way to prevent that is to consult the list of common offenders provided above.\nMake an option explicit\nIf you want depend on an option or locale, make sure it’s an explicit argument. Such arguments generally should not affect computation (Chapter 21), just side-effects like printed output or status messages. If they do affect results, follow Chapter 20 to make sure the user knows what’s happening. For example, lets take as.POSIXct() which basically looks something like this:\n\nas.POSIXct &lt;- function(x, tz = \"\") {\n  base::as.POSIXct(x, tz = tz)\n}\nas.POSIXct(\"2020-01-01 09:00\")\n#&gt; [1] \"2020-01-01 09:00:00 UTC\"\n\nThe tz argument is present, but it’s not obvious that \"\" means the current time zone. Let’s first make that explicit:\n\nas.POSIXct &lt;- function(x, tz = Sys.timezone()) {\n  base::as.POSIXct(x, tz = tz)\n}\nas.POSIXct(\"2020-01-01 09:00\")\n#&gt; [1] \"2020-01-01 09:00:00 UTC\"\n\nSince this is an important default whose value can change, we also print it out if the user hasn’t explicitly set it:\n\nas.POSIXct &lt;- function(x, tz = Sys.timezone()) {\n  if (missing(tz)) {\n    message(\"Using `tz = \\\"\", tz, \"\\\"`\")\n  }\n  base::as.POSIXct(x, tz = tz)\n}\nas.POSIXct(\"2020-01-01 09:00\")\n#&gt; Using `tz = \"UTC\"`\n#&gt; [1] \"2020-01-01 09:00:00 UTC\"\n\nSince most people don’t like lots of random output this provides a subtle incentive to supply the timezone:\n\nas.POSIXct(\"2020-01-01 09:00\", tz = \"America/Chicago\")\n#&gt; [1] \"2020-01-01 09:00:00 CST\"\n\nTemporarily adjust global state\nIf you’re calling a function with implicit arguments and those implicit arguments are causing problems with your code, you can always work around them by temporarily changing the global state which it uses. The easiest way to do so is to use the withr package, which provides a variety of tools to change temporarily change global state."
  },
  {
    "objectID": "inputs-explicit.html#see-also",
    "href": "inputs-explicit.html#see-also",
    "title": "Make inputs explicit",
    "section": "See also",
    "text": "See also\n\n\nChapter 21 and Chapter 20: how to make an option as explicit as possible.\n\nChapter 33: where a function changes global state in a surprising way."
  },
  {
    "objectID": "inputs-explicit.html#footnotes",
    "href": "inputs-explicit.html#footnotes",
    "title": "Make inputs explicit",
    "section": "",
    "text": "Such as data.frame(), as.data.frame(), and read.csv()↩︎\nPredictions and other diagnostics won’t be affected, but you’re likely to be surprised that your coefficients are different.↩︎"
  },
  {
    "objectID": "important-args-first.html#whats-the-pattern",
    "href": "important-args-first.html#whats-the-pattern",
    "title": "Put the most important arguments first",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nIn a function call, the most important arguments should come first. As a general rule, the most important arguments will be the ones that are used most often, but that’s often hard to tell until your function has existed in the wild for a while. Fortunately, there are a few rules of thumb that can help:\n\nIf the output is a transformation of an input (e.g. log(), stringr::str_replace(), dplyr::left_join()) then that argument the most important.\nOther arguments that determine the type or shape of the output are typically very important.\nOptional arguments (i.e. arguments with a default) are the least important, and should come last.\n\nThis convention makes it easy to understand the structure of a function at a glance: the more important an argument is, the earlier you’ll see it. When the output is very strongly tied to an input, putting that argument first also ensures that your function works well with the pipe, leading to code that focuses on the transformations rather than the object being transformed."
  },
  {
    "objectID": "important-args-first.html#what-are-some-examples",
    "href": "important-args-first.html#what-are-some-examples",
    "title": "Put the most important arguments first",
    "section": "What are some examples?",
    "text": "What are some examples?\nThe vast majority of functions get this right, so we’ll pick on a few examples which I think get it wrong:\n\nI think the arguments to base R string functions (grepl(), gsub(), etc) are in the wrong order because they consistently make the regular expression (pattern) the first argument, rather than the character vector being manipulated (x).\nThe first two arguments to lm() are formula and data. I’d argue that data should be the first argument; while it doesn’t affect the shape of the output which is always an lm S3 object, it does affect the shape of many important functions like predict(). However, the designers of lm() wanted data to be optional, so you could still fit models even if you hadn’t collected the individual variables into a data frame. Because formula is required and data is not, this means that formula had to come first.\n\nThe first two arguments to ggplot() are data and mapping. Both data and mapping are required for every plot, so why make data first? I picked this ordering because in most plots there’s one dataset shared across all layers and only the mapping changes.\nOn the other hand, the layer functions, like geom_point(), flip the order of these arguments because in an individual layer you’re more likely to specify mapping than data, and in many cases if you do specify data you’ll want mapping as well. This makes these the argument order inconsistent with ggplot(), but overall supports the most common use cases.\n\nggplot2 functions work by creating an object that is then added on to a plot, so the plot, which is really the most important argument, is not obvious at all. ggplot2 works this way in part because it was written before the pipe was discovered, and the best way I came up to define plots from left to right was to rely on + (so-called operator overloading). As an interesting historical fact, ggplot (the precursor to ggplot2) actually works great with the pipe, and a couple of years ago I bought it back to life as ggplot1."
  },
  {
    "objectID": "important-args-first.html#how-do-i-remediate-past-mistakes",
    "href": "important-args-first.html#how-do-i-remediate-past-mistakes",
    "title": "Put the most important arguments first",
    "section": "How do I remediate past mistakes?",
    "text": "How do I remediate past mistakes?\nGenerally, it is not possible to change the order of the first few arguments because it will break existing code (since these are the arguments that are mostly likely to be used unnamed). This means that the only real solution is to dperecate the entire function and replace it with a new one. Because this is invasive to the user, it’s best to do sparingly: if the mistake is minor, you’re better off waiting until you’ve collected other problems before fixing it. For example, take tidyr::gather(). It has a number of problems with its design, including the argument order, that makes it harder to use. Because it wasn’t possible to easily fix this mistake, we accumulated other gather() problems for several years before fixing them all at once in pivot_longer()."
  },
  {
    "objectID": "important-args-first.html#see-also",
    "href": "important-args-first.html#see-also",
    "title": "Put the most important arguments first",
    "section": "See also",
    "text": "See also\n\n\nChapter 8: If the function uses …, it should come in between the required and optional arguments."
  },
  {
    "objectID": "required-no-defaults.html#whats-the-pattern",
    "href": "required-no-defaults.html#whats-the-pattern",
    "title": "Required args shouldn’t have defaults",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nRequired arguments shouldn’t have defaults; optional arguments should have defaults. In other words, an argument should have a default if and only if it’s optional.\nThis simple convention ensures that you can tell which arguments are optional and which arguments are required from a glance at the function signature. Otherwise you need to rely on a careful reading of documentation. Additionally, if you don’t follow this convention and want to provide helpful error messages, you’ll need to implement them yourself rather than relying on R’s defaults.\n\n\n\n\n\n\nWhen should an argument be required?\n\n\n\n\n\nThis pattern raises the question of when an argument should be required, and when you should provide a default. I think this usually seems “obvious” but I wanted to discuss a few functions that might get it wrong:\n\nrnorm() and runif() are interesting cases as they set default values for mean/sd and min/max. Giving them defaults makes them feels like less important, and inconsistent with the other RNGs which generally require that you specify the parameters of the distribution. But both the normal and uniform distributions have very high-profile “standard” versions that make sense as defaults.\n\nYou can use predict() directly on a model and it gives predictions for the data used to fit the model:\n\nmod &lt;- lm(Employed ~ ., data = longley)\nhead(predict(mod))\n#&gt;     1947     1948     1949     1950     1951     1952 \n#&gt; 60.05566 61.21601 60.12471 61.59711 62.91129 63.88831\n\nIn my opinion, predict() should always require a dataset because prediction is primary about applying the model to new situations.\n\nstringr::str_sub() has default values for start and end. This allows you to do clever things like str_sub(x, end = 3) or str_sub(x, -3) to select the first or last three characters, but I now believe that leads to code that is harder to read, and it would have been better to make start and end required arguments."
  },
  {
    "objectID": "required-no-defaults.html#what-are-some-examples",
    "href": "required-no-defaults.html#what-are-some-examples",
    "title": "Required args shouldn’t have defaults",
    "section": "What are some examples?",
    "text": "What are some examples?\nThis is a straightforward convention that the vast majority of functions follow. There are a few exceptions that exist in base R, mostly for historical reasons. Here are a couple of examples:\n\n\nIn sample() neither x not size has a default value:\n\nargs(sample)\n#&gt; function (x, size, replace = FALSE, prob = NULL) \n#&gt; NULL\n\nThis suggests that size is required, but it’s actually optional:\n\nsample(1:4)\n#&gt; [1] 3 4 2 1\nsample(4)\n#&gt; [1] 1 4 2 3\n\n\n\nlm() does not have defaults for formula, data, subset, weights, na.action, or offset.\n\nargs(lm)\n#&gt; function (formula, data, subset, weights, na.action, method = \"qr\", \n#&gt;     model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, \n#&gt;     contrasts = NULL, offset, ...) \n#&gt; NULL\n\nBut only formula is actually required:\n\nx &lt;- 1:5\ny &lt;- 2 * x + 1 + rnorm(length(x))\nlm(y ~ x)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;      0.1599       2.3243\n\n\n\nIn the tidyverse, one function that fails to follow this pattern is ggplot2::geom_abline(), slope and intercept don’t have defaults but are not required. If you don’t supply them they default to slope = 1 and intercept = 0, or are taken from aes() if they’re provided there. This is a mistake caused by trying to have geom_abline() do too much — it can be both used as an annotation (i.e. with a single slope and intercept) or used to draw multiple lines from data (i.e. with one line for each row)."
  },
  {
    "objectID": "required-no-defaults.html#how-do-i-use-the-pattern",
    "href": "required-no-defaults.html#how-do-i-use-the-pattern",
    "title": "Required args shouldn’t have defaults",
    "section": "How do I use the pattern?",
    "text": "How do I use the pattern?\nThis pattern is generally easy to follow: if you don’t use missing() it’s very hard to do this by mistake."
  },
  {
    "objectID": "required-no-defaults.html#how-do-i-remediate-past-mistakes",
    "href": "required-no-defaults.html#how-do-i-remediate-past-mistakes",
    "title": "Required args shouldn’t have defaults",
    "section": "How do I remediate past mistakes?",
    "text": "How do I remediate past mistakes?\nIf an argument is required, remove the default argument. If an argument is optional, either set it to the default value, or if the computation is complicated, set it to NULL and then compute inside the body of the function."
  },
  {
    "objectID": "dots-after-required.html#whats-the-pattern",
    "href": "dots-after-required.html#whats-the-pattern",
    "title": "Put … after required arguments",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nIf you use … in a function, put it after the required arguments and before the optional arguments.\nThis has two positive impacts:\n\nIt forces the user of your function to fully name optional arguments, because arguments that come after ... are never matched by position or by partial name. We believe that using full names for optional arguments is good practice because it makes code easier to read.\nThis in turn means that uou can easily add new optional arguments or change the order of existing arguments without affecting existing code."
  },
  {
    "objectID": "dots-after-required.html#what-are-some-examples",
    "href": "dots-after-required.html#what-are-some-examples",
    "title": "Put … after required arguments",
    "section": "What are some examples?",
    "text": "What are some examples?\nThe arguments to mean() are x, trim, na.rm and …. This means that you can write code like this:\n\nx &lt;- c(1, 2, 10, NA)\nmean(x, , TRUE)\n#&gt; [1] 4.333333\nmean(x, n = TRUE, t = 0.1)\n#&gt; [1] 4.333333\n\nNot only does this allow for confusing code1, it also makes it hard to later change the order of these arguments, or introduce new arguments that might be more important.\nIf mean() instead placed … before trim and na.rm, like mean2()2 below, then you must fully name each argument:\n\nmean2 &lt;- function(x, ..., na.rm = FALSE, trim = 0) {\n  mean(x, ..., na.rm = na.rm, trim = trim)\n}\n\nmean2(x, na.rm = TRUE)\n#&gt; [1] 4.333333\nmean2(x, na.rm = TRUE, trim = 0.1)\n#&gt; [1] 4.333333"
  },
  {
    "objectID": "dots-after-required.html#how-do-i-remediate-past-mistakes",
    "href": "dots-after-required.html#how-do-i-remediate-past-mistakes",
    "title": "Put … after required arguments",
    "section": "How do I remediate past mistakes?",
    "text": "How do I remediate past mistakes?\nIt’s straightforward to fix a function where you’ve put ... in the wrong place: you just need to change the argument order and use rlang::check_dots_used() to check that no arguments are lost (learn more in Chapter 24). This is a breaking change, but it tends to affect relatively little code because most people do fully name optional arguments.\nWe can use this approach to make a safer version of mean():\n\nmean3 &lt;- function(x, ..., na.rm = FALSE, trim = 0) {\n  rlang::check_dots_used()\n  mean(x, ..., na.rm = na.rm, trim = trim)\n}\n\nmean3(x, , TRUE)\n#&gt; Error in `mean3()`:\n#&gt; ! Arguments in `...` must be used.\n#&gt; ✖ Problematic argument:\n#&gt; • ..1 = TRUE\n#&gt; ℹ Did you misspell an argument name?\n\nmean3(x, n = TRUE, t = 0.1)\n#&gt; Error in `mean3()`:\n#&gt; ! Arguments in `...` must be used.\n#&gt; ✖ Problematic arguments:\n#&gt; • n = TRUE\n#&gt; • t = 0.1\n#&gt; ℹ Did you misspell an argument name?\n\n\n\n\n\n\n\nBase R\n\n\n\n\n\nIn base R you can use base::chkDots(), but it uses a slightly simpler technique which means it’s not suitable for usage in S3 methods."
  },
  {
    "objectID": "dots-after-required.html#see-also",
    "href": "dots-after-required.html#see-also",
    "title": "Put … after required arguments",
    "section": "See also",
    "text": "See also\n\n\nChapter 22: if … is a required argument because it’s used to combine an arbitrary number of objects in a data structure.\n\nChapter 24: to ensure that arguments to … never go silently missing."
  },
  {
    "objectID": "dots-after-required.html#footnotes",
    "href": "dots-after-required.html#footnotes",
    "title": "Put … after required arguments",
    "section": "",
    "text": "As much as we recommended people don’t write code like this, you know someone will!↩︎\nNote that I moved na.rm = TRUE in front of trim because I believe na.rm is the more important argument because it’s used vastly more often than trim and I’m following Chapter 6.↩︎"
  },
  {
    "objectID": "defaults-short-and-sweet.html#whats-the-pattern",
    "href": "defaults-short-and-sweet.html#whats-the-pattern",
    "title": "Keep defaults short and sweet",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nDefault values should be short and sweet. Avoid large or complex calculations in the default values, instead using NULL or a helper function when the default requires complex calculation. This keeps the function specification focussed on the big picture (i.e. what are the arguments and are they required or not) rather than the details of the defaults."
  },
  {
    "objectID": "defaults-short-and-sweet.html#what-are-some-examples",
    "href": "defaults-short-and-sweet.html#what-are-some-examples",
    "title": "Keep defaults short and sweet",
    "section": "What are some examples?",
    "text": "What are some examples?\nIt’s common for functions to use NULL to mean that the argument is optional, but the computation of the default is non-trivial:\n\nThe default label in cut() yields labels in the form [a, b).\nThe default pattern in dir() means match all files.\nThe default by in dplyr::left_join() means join using the common variables between the two data frames (the so-called natural join).\nThe default mapping in ggplot2::geom_point() (and friends) means use the mapping from in the overall plot.\n\nIn other cases, we encapsulate default values into a function:\n\nreadr functions use a family of functions including readr::show_progress(), readr::should_show_col_types() and readr::should_show_lazy() that make it easier for users to override various defaults.\n\nIt’s also worth looking at a couple of counter examples that come from base R:\n\nThe default value for by in seq is ((to - from)/(length.out - 1)).\n\nreshape() has a very long default argument: the split argument is one of two possible lists depending on the value of the sep argument:\n\nreshape &lt;- function(\n    ...,\n    split = if (sep == \"\") {\n      list(regexp = \"[A-Za-z][0-9]\", include = TRUE)\n    } else {\n      list(regexp = sep, include = FALSE, fixed = TRUE)\n    }\n) {}\n\n\nsample.int() uses a complicated rule to determine whether or not to use a faster hash based method that’s only applicable in some circumstances: useHash = (!replace && is.null(prob) && size &lt;= n/2 && n &gt; 1e+07))."
  },
  {
    "objectID": "defaults-short-and-sweet.html#how-do-i-use-it",
    "href": "defaults-short-and-sweet.html#how-do-i-use-it",
    "title": "Keep defaults short and sweet",
    "section": "How do I use it?",
    "text": "How do I use it?\nSo what should you do if a default requires some complex calculation? We have two recommended approaches: using NULL or creating a helper function. I’ll also show you two other alternatives which we don’t generally recommend but you’ll see in a handful of places in the tidyverse, and can be useful in limited circumstances.\n\nNULL default\nThe simplest, and most common, way to indicate that an argument is optional, but has a complex default is to use NULL as the default. Then in the body of the function you perform the actual calculation only if the is NULL. For example, if we were to use this approach in sample.int(), it might look something like this:\n\nsample.int &lt;- function (n, size = n, replace = FALSE, prob = NULL, useHash = NULL)  {\n  if (is.null(useHash)) {\n    useHash &lt;- n &gt; 1e+07 && !replace && is.null(prob) && size &lt;= n/2\n  }\n}\n\nThis pattern is made more elegant with the infix %||% operator which is built in to R 4.4. If you need it in an older version of R you can import it from rlang or copy and paste it in to your utils.R:\n\n`%||%` &lt;- function(x, y) if (is.null(x)) y else x\n\nsample.int &lt;- function (n, size = n, replace = FALSE, prob = NULL, useHash = NULL)  {\n  useHash &lt;- useHash %||% n &gt; 1e+07 && !replace && is.null(prob) && size &lt;= n/2\n}\n\n%||% is particularly well suited to arguments where the default value is found through a cascading system of fallbacks. For example, this code from ggplot2::geom_bar() finds the width by first looking at the data, then in the parameters, finally falling back to computing it from the resolution of the x variable:\n\nwidth &lt;- data$width %||% params$width %||% (resolution(data$x, FALSE) * 0.9)\n\nDon’t use %||% for more complex examples where the individual clauses can’t fit on their own line. For example in reshape(), I wouldn’t write:\n\nreshape &lt;- function(..., sep = \".\", split = NULL) {\n  split &lt;- split %||% if (sep == \"\") {\n    list(regexp = \"[A-Za-z][0-9]\", include = TRUE)\n  } else {\n    list(regexp = sep, include = FALSE, fixed = TRUE)\n  }  \n  ...\n}\n\nI would instead use is.null() and assign split inside each branch:\n\nreshape &lt;- function(..., sep = \".\", split = NULL) {\n  if (is.null(split)) {\n    if (sep == \"\") {\n      split &lt;- list(regexp = \"[A-Za-z][0-9]\", include = TRUE)\n    } else {\n      split &lt;- list(regexp = sep, include = FALSE, fixed = TRUE)\n    }\n  }\n  ...\n}\n\nOr alternatively you might pull the code out into a helper function:\n\nsplit_default &lt;- function(sep = \".\") {\n if (sep == \"\") {\n    list(regexp = \"[A-Za-z][0-9]\", include = TRUE)\n  } else {\n    list(regexp = sep, include = FALSE, fixed = TRUE)\n  }\n}\n\nreshape &lt;- function(..., sep = \".\", split = NULL) {\n  split &lt;- split %||% split_default(sep)\n  ...\n}\n\nThat makes it very clear exactly which other arguments the default for split depends on.\nExported helper function\nIf you have created a helper function for your own use, might consider use it as the default:\n\nreshape &lt;- function(..., sep = \".\", split = split_default(sep)) {\n  ...\n}\n\nThe problem with using an internal function as the default is that the user can’t easily run this function to see what it does, making the default a bit magical (Chapter 19). So we recommend that if you want to do this you export and document that function. This is the main downside of this approach: you have to think carefully about the name of the function because it’s user facing.\nA good example of this pattern is readr::show_progress(): it’s used in every read_ function in readr to determine whether or not a progress bar should be shown. Because it has a relatively complex explanation, it’s nice to be able to document it in its own file, rather than cluttering up file reading functions with incidental details.\nAlternatives\nIf the above techniques don’t work for your case there are two other alternatives that we don’t generally recommend but can be useful in limited situations.\n\n\n\n\n\n\nSentinel value\n\n\n\n\n\nSometimes you’d like to use the NULL approach defined above, but NULL already has a specific meaning that you want to preserve. For example, this comes up in ggplot2 scales functions which allow you to set the name of the scale which is displayed on the axis or legend. The default value should just preserve whatever existing label is present so that if you’re providing a scale to customise (e.g.) the breaks or labels, you don’t need to re-type the scale name. However, NULL is also a meaningful value because it means eliminate the scale label altogether1. For that reason the default value for name is ggplot2::waiver() a ggplot2-specific convention that means “inherit from the existing value”.\nIf you look at ggplot2::waiver() you’ll see it’s just a very lightweight S3 class2:\n\nggplot2::waiver\n#&gt; function () \n#&gt; structure(list(), class = \"waiver\")\n#&gt; &lt;bytecode: 0x557995d4e608&gt;\n#&gt; &lt;environment: namespace:ggplot2&gt;\n\nAnd then ggplot2 also provides the internal is.waive()3 function which allows to work with it in the same way we might work with a NULL:\n\nis.waive &lt;- function(x) {\n  inherits(x, \"waiver\")\n}\n\nThe primary downside of this technique is that it requires substantial infrastructure to set up, so it’s only really worth it for very important functions or if you’re going to use it in multiple places.\n\n\n\n\n\n\n\n\n\nNo default\n\n\n\n\n\nThe final alternative is to condition on the absence of an argument using missing(). It works something like this:\n\nreshape &lt;- function(..., sep = \".\", split) {\n  if (missing(split)) {\n    split &lt;- split_default(sep)\n  }\n  ...\n}\n\nI mention this technique because we used it in purrr::reduce() for the .init argument. This argument is mostly optional:\n\nlibrary(purrr)\n#&gt; \n#&gt; Attaching package: 'purrr'\n#&gt; The following object is masked _by_ '.GlobalEnv':\n#&gt; \n#&gt;     %||%\nreduce(letters[1:3], paste)\n#&gt; [1] \"a b c\"\nreduce(letters[1:2], paste)\n#&gt; [1] \"a b\"\nreduce(letters[1], paste)\n#&gt; [1] \"a\"\n\nBut it is required when .x (the first argument) is empty, and it’s good practice to supply it when wrapping reduce() inside another function because it ensures that you get the right type of output for all inputs:\n\nreduce(letters[0], paste)\n#&gt; Error in `reduce()`:\n#&gt; ! Must supply `.init` when `.x` is empty.\nreduce(letters[0], paste, .init = \"\")\n#&gt; [1] \"\"\n\nWhy use this approach? NULL is a potentially valid option for .init, so we can’t use that approach. And we only need it for a single function, that’s not terribly important, so creating a sentinel didn’t seem to worth it. .init is “semi” required so this seemed to be the least worst solution to the problem.\nThe major drawback to this technique is that it makes it look like an argument is required (in direct conflict with Chapter 7)."
  },
  {
    "objectID": "defaults-short-and-sweet.html#how-do-i-remediate-existing-problems",
    "href": "defaults-short-and-sweet.html#how-do-i-remediate-existing-problems",
    "title": "Keep defaults short and sweet",
    "section": "How do I remediate existing problems?",
    "text": "How do I remediate existing problems?\nIf you have a function with a long default, you can remediate it with any of the approaches. It won’t be a breaking change unless you accidentally change the computation of the default, so make sure you have a test for that before you begin."
  },
  {
    "objectID": "defaults-short-and-sweet.html#see-also",
    "href": "defaults-short-and-sweet.html#see-also",
    "title": "Keep defaults short and sweet",
    "section": "See also",
    "text": "See also\n\nSee Chapter 11 for a tecnhnique to simplify your function spec if its long because it has many less important optional arguments."
  },
  {
    "objectID": "defaults-short-and-sweet.html#footnotes",
    "href": "defaults-short-and-sweet.html#footnotes",
    "title": "Keep defaults short and sweet",
    "section": "",
    "text": "Unlike name = \"\" which doesn’t show the label, but preserves the space where it would appear (sometimes useful for aligning multiple plots), name = NULL also eliminates the space normally allocated for the label.↩︎\nIf I was to write this code today I’d use ggplot2_waiver as the class name.↩︎\nIf I wrote this code today, I’d call it is_waiver().↩︎"
  },
  {
    "objectID": "enumerate-options.html#whats-the-pattern",
    "href": "enumerate-options.html#whats-the-pattern",
    "title": "Enumerate possible options",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nIf the possible values of an argument are a small set of strings, set the default argument to the set of possible values, and then use match.arg() or rlang::arg_match() in the function body. This convention advertises to the knowledgeable user1 what the possible values, and makes it easy to generate an informative error message for inappropriate inputs. This interface is often coupled with an implementation that uses switch().\nThis convention makes it possible to advertise the possible set of values for an argument. The advertisement happens in the function specification, so you see in tool tips and autocomplete, without having to look at the documentation."
  },
  {
    "objectID": "enumerate-options.html#what-are-some-examples",
    "href": "enumerate-options.html#what-are-some-examples",
    "title": "Enumerate possible options",
    "section": "What are some examples?",
    "text": "What are some examples?\n\nIn difftime(), units can be any one of “auto”, “secs”, “mins”, “hours”, “days”, or “weeks”.\nIn format(), justify can be “left”, “right”, “center”, or “none”.\nIn trimws(), you can choose which side to remove whitespace from: “both”, “left”, or “right”.\nIn rank(), you can select the ties.method from one of “average”, “first”, “last”, “random”, “max”, or “min”.\nrank() exposes six different methods for handling ties with the ties.method argument.\nquantile() exposes nine different approaches to computing a quantile through the type argument.\np.adjust() exposes eight strategies for adjusting P values to account for multiple comparisons using the p.adjust.methods argument."
  },
  {
    "objectID": "enumerate-options.html#how-do-i-use-this-pattern",
    "href": "enumerate-options.html#how-do-i-use-this-pattern",
    "title": "Enumerate possible options",
    "section": "How do I use this pattern?",
    "text": "How do I use this pattern?\nTo use this technique, set the default value to a character vector, where the first value is the default. Inside the function, use match.arg() or rlang::arg_match() to check that the value comes from the known good set, and pick the default if none is supplied.\nTake rank(), for example. The heart of its implementation looks like this:\n\nrank &lt;- function(\n    x,\n    ties.method = c(\"average\", \"first\", \"last\", \"random\", \"max\", \"min\")\n) {\n  \n  ties.method &lt;- match.arg(ties.method)\n  \n  switch(ties.method, \n    average = , \n    min = , \n    max = .Internal(rank(x, length(x), ties.method)), \n    first = sort.list(sort.list(x)),\n    last = sort.list(rev.default(sort.list(x, decreasing = TRUE))), \n    random = sort.list(order(x, stats::runif(length(x))))\n  )\n}\n\nx &lt;- c(1, 2, 2, 3, 3, 3)\n\nrank(x)\n#&gt; [1] 1.0 2.5 2.5 5.0 5.0 5.0\nrank(x, ties.method = \"first\")\n#&gt; [1] 1 2 3 4 5 6\nrank(x, ties.method = \"min\")\n#&gt; [1] 1 2 2 4 4 4\n\nNote that match.arg() will automatically throw an error if the value is not in the set:\n\nrank(x, ties.method = \"middle\")\n#&gt; Error in match.arg(ties.method): 'arg' should be one of \"average\", \"first\", \"last\", \"random\", \"max\", \"min\"\n\nIt also supports partial matching so that the following code is shorthand for ties.method = \"random\":\n\nrank(x, ties.method = \"r\")\n#&gt; [1] 1 3 2 5 6 4\n\nWe prefer to avoid partial matching because while it saves a little time writing the code, it makes reading the code less clear. rlang::arg_match() is an alternative to match.arg() that doesn’t support partial matching. It instead provides a helpful error message:\n\nrank2 &lt;- function(\n    x,\n    ties.method = c(\"average\", \"first\", \"last\", \"random\", \"max\", \"min\")\n) {\n  ties.method &lt;- rlang::arg_match(ties.method)\n  rank(x, ties.method = ties.method)\n}\n\nrank2(x, ties.method = \"r\")\n#&gt; Error in `rank2()`:\n#&gt; ! `ties.method` must be one of \"average\", \"first\", \"last\", \"random\",\n#&gt;   \"max\", or \"min\", not \"r\".\n#&gt; ℹ Did you mean \"random\"?\n\n# It also provides a suggestion if you misspell the argument\nrank2(x, ties.method = \"avarage\")\n#&gt; Error in `rank2()`:\n#&gt; ! `ties.method` must be one of \"average\", \"first\", \"last\", \"random\",\n#&gt;   \"max\", or \"min\", not \"avarage\".\n#&gt; ℹ Did you mean \"average\"?\n\nEscape hatch\nIt’s sometimes useful to build in an escape hatch from canned strategies. This allows users to access alternative strategies, and allows for experimentation that can later turn into a official strategies. One example of such an escape hatch is in name repair, which occurs in many places throughout the tidyverse. One place you might encounter it is in tibble():\n\ntibble::tibble(a = 1, a = 2)\n#&gt; Error in `tibble::tibble()`:\n#&gt; ! Column name `a` must not be duplicated.\n#&gt; Use `.name_repair` to specify repair.\n#&gt; Caused by error in `repaired_names()`:\n#&gt; ! Names must be unique.\n#&gt; ✖ These names are duplicated:\n#&gt;   * \"a\" at locations 1 and 2.\n\nBeneath the surface all tidyverse functions that expose some sort of name repair eventually end up calling vctrs::vec_as_names():\n\nvctrs::vec_as_names(c(\"a\", \"a\"), repair = \"check_unique\")\n#&gt; Error:\n#&gt; ! Names must be unique.\n#&gt; ✖ These names are duplicated:\n#&gt;   * \"a\" at locations 1 and 2.\nvctrs::vec_as_names(c(\"a\", \"a\"), repair = \"unique\")\n#&gt; New names:\n#&gt; • `a` -&gt; `a...1`\n#&gt; • `a` -&gt; `a...2`\n#&gt; [1] \"a...1\" \"a...2\"\nvctrs::vec_as_names(c(\"a\", \"a\"), repair = \"unique_quiet\")\n#&gt; [1] \"a...1\" \"a...2\"\n\nvec_as_names() exposes six strategies, but it also allows you to supply a function:\n\nvctrs::vec_as_names(c(\"a\", \"a\"), repair = toupper)\n#&gt; [1] \"A\" \"A\"\n\nHow keep defaults short?\nThis technique is a best used when the set of possible values is short as otherwise you run the risk of dominating the function spec with this one argument (Chapter 9). If you have a long list of possibilities, there are three possible solutions:\n\n\nSet a single default and supply the possible values to match.arg()/arg_match():\n\nrank2 &lt;- function(x, ties.method = \"average\") {\n  ties.method &lt;- arg_match(\n    ties.method, \n    c(\"average\", \"first\", \"last\", \"random\", \"max\", \"min\")\n  )\n}\n\n\n\nIf the values are used by many functions, you can store the options in an exported vector:\n\nties.methods &lt;- c(\"average\", \"first\", \"last\", \"random\", \"max\", \"min\")\n\nrank2 &lt;- function(x, ties.method = ties.methods) {\n  ties.method &lt;- arg_match(ties.method)\n}\n\nFor example stats::p.adjust(), stats::pairwise.prop.test(), stats::pairwise.t.test(), stats::pairwise.wilcox.test() all use p.adjust.method = p.adjust.methods.\n\n\nYou can store the options in a exported named list2. That has the advantage that you can advertise both the source of the values, and the defaults, and the user gets a nice auto-complete of the possible values.\n\nlibrary(rlang)\nties &lt;- as.list(set_names(c(\"average\", \"first\", \"last\", \"random\", \"max\", \"min\")))\n\nrank2 &lt;- function(x, ties.method = ties$average) {\n  ties.method &lt;- arg_match(ties.method, names(ties))\n}"
  },
  {
    "objectID": "enumerate-options.html#footnotes",
    "href": "enumerate-options.html#footnotes",
    "title": "Enumerate possible options",
    "section": "",
    "text": "The main downside of this technique is that many users aren’t aware of this convention and that the first value of the vector will be used as a default.↩︎\nThanks to Brandon Loudermilk↩︎"
  },
  {
    "objectID": "argument-clutter.html#whats-the-problem",
    "href": "argument-clutter.html#whats-the-problem",
    "title": "Reduce argument clutter with an options object",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nIf you have a large number of optional arguments that control the fine details of the operation of a function, it might be worth lumping them all together into a separate “options” object created by a helper function.\nHaving a large number of less important arguments makes it harder to see the most important. By moving rarely used and less important arguments to a secondary function, you can more easily draw attention to what is most important."
  },
  {
    "objectID": "argument-clutter.html#what-are-some-examples",
    "href": "argument-clutter.html#what-are-some-examples",
    "title": "Reduce argument clutter with an options object",
    "section": "What are some examples?",
    "text": "What are some examples?\n\n\nMany base R modelling functions like loess(), glm(), and nls() have a control argument that are paired with a function like loess.control(), glm.control(), and nls.control(). These allow you to modify rarely used defaults, including the number of iterations, the stopping criteria, and some debugging options.\noptim() uses a less formal version of this structure — while it has a control argument, it doesn’t have a matching optim.control() helper. Instead, you supply a named list with components described in ?optim. A helper function is more convenient than a named list because it checks the argument names for free and gives nicer autocomplete to the user.\n\nThis pattern is common in other modelling packages, e.g. tune::fit_resamples() + tune::control_resamples(), tune::control_bayes(), tune::control_grid(), and caret::train() + caret::trainControl()\nreadr::read_delim() and friends take a locale argument which is paired with the readr::locale() helper. This object bundles together a bunch of options related to parsing numbers, dates, and times that vary from country to country.\nreadr::locale() itself has a date_names argument that’s paired with readr::date_names() and readr::date_names_lang() helpers. You typically use the argument by supplying a two letter locale (which date_names_lang() uses to look up common languages), but if your language isn’t supported you can use readr::date_names() to individually supply full and abbreviated month and day of week names.\n\nOn the other hand, some functions with many arguments that would benefit from this technique include:\n\nreadr::read_delim() has a lot of options that control rarely needed details of file parsing (e.g. escape_backslash, escape_double, quoted_na, comment, trim_ws). These make the function specification very long and might well be better in a details object.\nggplot2::geom_smooth() fits a smooth line to your data. Most of the time you only want to pick the model and formula used, but geom_smooth() (via ggplot2::stat_smooth()) also provides n, fullrange, span, level, and method.args to control details of the fit. I think these would be better in their own details object."
  },
  {
    "objectID": "argument-clutter.html#how-do-i-use-this-pattern",
    "href": "argument-clutter.html#how-do-i-use-this-pattern",
    "title": "Reduce argument clutter with an options object",
    "section": "How do I use this pattern?",
    "text": "How do I use this pattern?\nThe simplest implementation is just to write a helper function that returns a list:\n\nmy_fun_opts &lt;- function(opt1 = 1, opt2 = 2) {\n  list(\n    opt1 = opt1,\n    opt2 = opt2\n  )\n}\n\nThis alone is nice because you can document the individual arguments, you get name checking for free, and auto-complete will remind the user what these less important options include.\nBetter error messages\nAn optional extra is to add a unique class to the list:\n\nmy_fun_opts &lt;- function(opt1 = 1, opt2 = 2) {\n  structure(\n    list(\n      opt1 = opt1,\n      opt2 = opt2\n    ),\n    class = \"mypackage_my_fun_opts\"\n  )\n}\n\nThis then allows you to create more informative error messages:\n\nmy_fun_opts &lt;- function(..., opts = my_fun_opts()) {\n  if (!inherits(opts, \"mypackage_my_fun_opts\")) {\n    cli::cli_abort(\"{.arg opts} must be created by {.fun my_fun_opts}.\")\n  }\n}\n\nmy_fun_opts(opts = 1)\n#&gt; Error in `my_fun_opts()`:\n#&gt; ! `opts` must be created by `my_fun_opts()`.\n\nIf you use this option in many places, you should consider pulling out the repeated code into a check_my_fun_opts() function."
  },
  {
    "objectID": "argument-clutter.html#how-do-i-remediate-past-mistakes",
    "href": "argument-clutter.html#how-do-i-remediate-past-mistakes",
    "title": "Reduce argument clutter with an options object",
    "section": "How do I remediate past mistakes?",
    "text": "How do I remediate past mistakes?\nTypically you notice this problem only after you have created too many options so you’ll need to carefully remediate by introducing a new options argument and paired helper function. For example, if your existing function looks like this:\n\nmy_fun &lt;- function(x, y, opt1 = 1, opt2 = 2) {\n  \n}\n\nIf you want to keep the existing function specification you could add a new opts argument that uses the values of opt1 and opt2:\n\nmy_fun &lt;- function(x, y, opts = NULL, opt1 = 1, opt2 = 2) {\n  \n  opts &lt;- opts %||% my_fun_opts(opt1 = opt1, opt2 = opt2)\n}\n\nHowever, that introduces a dependency between the arguments: if you specify both opts and opt1/opt2, opts will win. You could certainly add extra code to pick up on this problem and warn the user, but I think it’s just cleaner to deprecate the old arguments so that you can eventually remove them:\n\nmy_fun &lt;- function(x, y, opts = my_fun_opts(), opt1 = deprecated(), opt2 = deprecated()) {\n  \n  if (lifecycle::is_present(opt1)) {\n    lifecycle::deprecate_warn(\"1.0.0\", \"my_fun(opt1)\", \"my_fun_opts(opt1)\")\n    opts$opt1 &lt;- opt1\n  }\n  if (lifecycle::is_present(opt2)) {\n    lifecycle::deprecate_warn(\"1.0.0\", \"my_fun(opt2)\", \"my_fun_opts(opt2)\")\n    opts$opt2 &lt;- opt2\n  }\n}\n\nThen you can remove the old arguments in a future release."
  },
  {
    "objectID": "argument-clutter.html#see-also",
    "href": "argument-clutter.html#see-also",
    "title": "Reduce argument clutter with an options object",
    "section": "See also",
    "text": "See also\n\n\nChapter 14 is a similar pattern when you have multiple options function that each encapsulate a different strategy."
  },
  {
    "objectID": "independent-meaning.html#whats-the-problem",
    "href": "independent-meaning.html#whats-the-problem",
    "title": "Argument meaning should be independent",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nAvoid having one argument change the interpretation of another argument. This makes it harder to understand a call, because you might need to re-intepret an earlier argument. This sort of call is like a garden path sentence, like “The horse raced past the barn fell” where your initial understanding of”raced” needs to be modified when you to get to the end of the sentence in order for it to make sense."
  },
  {
    "objectID": "independent-meaning.html#what-are-some-examples",
    "href": "independent-meaning.html#what-are-some-examples",
    "title": "Argument meaning should be independent",
    "section": "What are some examples?",
    "text": "What are some examples?\nThere aren’t many examples of this problem, but it’s pretty rare. I think it’s still worth being aware of it so you can avoid in your own code.\n\n\nIn library() the character.only argument changes how the package argument is interpreted:\n\npackage &lt;- \"dplyr\"\n\n# Loads a package called \"package\"\nlibrary(package)\n\n# Loads dplyr\nlibrary(package, character.only = TRUE)\n\n\nIn install.packages() setting repos = NULL changes the interpretation of pkgs from being a vector of package names to a vector of file paths.\nIn findInterval() if you set left.open = TRUE the rightmost.closed means leftmost.closed."
  },
  {
    "objectID": "independent-meaning.html#how-do-i-remediate-past-mistakes",
    "href": "independent-meaning.html#how-do-i-remediate-past-mistakes",
    "title": "Argument meaning should be independent",
    "section": "How do I remediate past mistakes?",
    "text": "How do I remediate past mistakes?\nIt’s not clear that there’s a single solution. For the examples above:\n\nIn library(), I think this is an example of why base R needs a consistent mechanism for quoting and unquoting. If library() were a tidyverse function it would use tidy-eval, and so you’d write library(package) or library(!!package). Another option would be to use the same mechanism as help() where help((topic)) will always look for the topic named by the topic variable.\nIn install.packages() maybe it would be better to have mutually exclusive packages and paths arguments? Then repos only applies to paths which might suggest this is an example of the strategy pattern?\nIn findInterval() I think I’d fix it by renaming the argument to something that wasn’t direction specific. Maybe extemum.closed?"
  },
  {
    "objectID": "explicit-strategies.html#see-also",
    "href": "explicit-strategies.html#see-also",
    "title": "Strategies",
    "section": "See also",
    "text": "See also\n\nThe original strategy pattern defined in Design Patterns. This pattern has a rather different implementation in a classic OOP language."
  },
  {
    "objectID": "boolean-strategies.html#whats-the-pattern",
    "href": "boolean-strategies.html#whats-the-pattern",
    "title": "Prefer a enum, even if only two choices",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nIf your function implements two strategies, it’s tempting to distinguish between them using an argument that takes either TRUE or FALSE. However, I recommend that you use an enumeration unless:\n\nYou’re really sure there won’t ever be another strategy. If you do discover a third (or fourth, or fifth, or …) strategy, you’ll need to change the interface of your function.\nIt’s very clear what both TRUE and FALSE options mean just from the name of the argument. Generally the TRUE value tends to be easier to understand because something = TRUE tells you what will happen, but something = FALSE only tells you what won’t happen."
  },
  {
    "objectID": "boolean-strategies.html#what-are-some-examples",
    "href": "boolean-strategies.html#what-are-some-examples",
    "title": "Prefer a enum, even if only two choices",
    "section": "What are some examples?",
    "text": "What are some examples?\nThere are quite a few examples of the problem in tidyverse, because this is a pattern that we only discovered relatively recently:\n\nBy default, stringr::str_subset(string, pattern) returns the elements of string that match the pattern. You can use negate = TRUE to instead return the elements that don’t match the pattern, but I now wonder if would be more clear as return = c(\"matches\", \"non-matches\").\n\nhttr2::multi_req_perform() allows you to perform a bunch of HTTP requests in parallel. It has an argument called cancel_on_error that can take TRUE or FALSE. It’s fairly clear what cancel_on_error = TRUE means; but it’s not so obvious what cancel_on_error = FALSE does. Additionally, it seems likely that I’ll come up with other possible error handling strategies in the future, and even though I don’t know what they are now, it would be better to plan for the future with an argument specification like error = c(\"cancel\", \"continue\").\n\ncut() has an argument called right which is used to pick between right-closed left-open intervals (TRUE) and right-open left-closed arguments. I think it’s hard to remember which is which and a clearer specification might be open_side = c(\"right\", \"left\") or maybe bounds = c(\"[)\", \"(]\"). Another interesting case in base R is sort() which has two arguments that take a single logical value: decreasing and na.last:\n\n\nThe decreasing argument is used to pick between sorting in ascending or descending order. It’s easy to understand what decreasing = TRUE does, but slightly less clear what decreasing = FALSE, the default, means because it feels like a double negative:\n\nx &lt;- sample(10)\nsort(x, decreasing = TRUE)\nsort(x, decreasing = FALSE)\n\nCompare this with vctrs::vec_sort(), which uses an enum:\n\nvctrs::vec_sort(x, direction = \"desc\")\nvctrs::vec_sort(x, direction = \"asc\")\n\nI think this is a mild improvement because the two options are spelled out explicitly.\n\nThe na.last argument is used to control the location of missing values in the result. It takes three possible values: TRUE (put NAs at the end), FALSE (put NAs at the beginning), or NA (drop NAs from the result). This is an interesting way to support three strategies, but as we’ll see later I think this would be more clear the argument specification was na = c(\"drop\", \"first\", \"last\")."
  },
  {
    "objectID": "boolean-strategies.html#how-do-you-remediate-past-mistakes",
    "href": "boolean-strategies.html#how-do-you-remediate-past-mistakes",
    "title": "Prefer a enum, even if only two choices",
    "section": "How do you remediate past mistakes?",
    "text": "How do you remediate past mistakes?\nThere are two possible ways to switch to using a strategy instead of TRUE/FALSE depending on whether the old argument name makes sense with the new argument values. The sections below show what you’ll need to do if you need a new argument (most cases) or if you’re lucky enough to be able to reuse the existing argument.\nCreate a new argument\nImagine we wanted to remediate the na.last argument to sort(). Currently:\n\n\nna.last = TRUE means put NAs last.\n\nna.last = FALSE means put NAs first.\n\nna.list = NA means to drop them.\n\nI think we could make this function more clear by changing the argument name to na and accepting one of three values: last, first, or drop.\nChanging an argument name is equivalent removing the old name and adding the new name. This way of thinking about the change makes it easier to see how you do it in a backward compatible way: you need to deprecate the old argument in favour of the new one.\n\nsort &lt;- function(x,\n                 na.last = lifecycle::deprecated(),\n                 na = c(\"drop\", \"first\", \"last\")) {\n  if (lifecycle::is_present(na.last)) {\n    lifecycle::deprecate_warn(\"1.0.0\", \"sort(na.last)\", \"sort(na)\")\n\n    if (!is.logical(na.last) || length(na.last) != 1) {\n      cli::cli_abort(\"{.arg na.last} must be a single TRUE, FALSE, or NA.\")\n    }\n    \n    if (isTRUE(na.last)) {\n      na &lt;- \"last\"\n    } else if (isFALSE(na.last)) {\n      na &lt;- \"first\"\n    } else {\n      na &lt;- \"drop\"\n    }\n  } else {\n    na &lt;- arg_match(na)\n  }\n  \n  ...\n}\n\n\n\n\n\n\n\nNote that because na is a prefix of na.last and sort() puts na.last before …,not after it (see Chapter 8), this introduces a very subtle behaviour change. Previously, sort(x, n = TRUE) would have worked and been equivalent to sort(x, na.last = TRUE). But it will now fail because n is a prefix of two arguments (na and na.last). This is unlikely to affect much code, but is worth being aware of.\nIt would also be nice to make the default value \"last\" to match order(), especially since it’s very unusual for a function to silently remove missing values. However, that’s likely to affect a lot of existing code, making it unlikely to be worthwhile.\n\n\n\nRe-use an existing name\nOriginally haven::write_sav(compress) could either be TRUE (compress the file) or FALSE (don’t compress it). But then SPSS version 21.0 introduced a new way of compressing files leading to three possible options: compress with the new way (zsav), compress with the old way (bytes), or don’t compress. In this case we got lucky because we can continue to use the same argument name: compress = c(\"byte\", \"zsav\", \"none\"). We allowed existing code by special casing the behaviour of TRUE and FALSE:\n\nwrite_sav &lt;- function(data, path, compress = c(\"byte\", \"zsav\", \"none\"), adjust_tz = TRUE) {\n  if (isTRUE(compress)) {\n    compress &lt;- \"zsav\"\n  } else if (isFALSE(compress)) {\n    compress &lt;- \"none\"\n  } else {\n    compress &lt;- arg_match(compress)\n  }\n\n  ...\n}\n\nYou could choose to deprecate TRUE and FALSE, but here we chose to the keep them since it’s only a small amount of extra code in haven, and it means that existing users don’t need to think about it. See ?haven::read_sav for how we communicated the change in the docs.\nIn a future version of haven we might change the order of the enum so that the zsav compression method becomes the default. This generally yields smaller files but can’t be read by older versions of SPSS. Now that v21 is over 5 years old1, it’s reasonable to make the smaller format the default."
  },
  {
    "objectID": "boolean-strategies.html#footnotes",
    "href": "boolean-strategies.html#footnotes",
    "title": "Prefer a enum, even if only two choices",
    "section": "",
    "text": "Five years is the general threshold for support across the tidyverse.↩︎"
  },
  {
    "objectID": "strategy-objects.html#whats-the-problem",
    "href": "strategy-objects.html#whats-the-problem",
    "title": "Extract strategies into objects",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nSometimes different strategies need different arguments. In this case, instead of using an enum, you’ll need to use richer objects capable of storing optional values as well as the strategy name.\nYou can also see this as an extension of Chapter 11\nA more advanced form of this pattern arises when you have many arguments apart from the arguments needed by the different strategies. One example that we’ll explore below is ggplot2::stat_bin(). A key part of stat_bin() (which powers ggplot2::geom_histogram() is defining the locations of the bins, which currently uses five argument (binwidth, bins, breaks, boundary, center, closed). But it also has many arguments (e.g. na.rm, orientation, show.legend, ...)"
  },
  {
    "objectID": "strategy-objects.html#what-are-some-examples",
    "href": "strategy-objects.html#what-are-some-examples",
    "title": "Extract strategies into objects",
    "section": "What are some examples?",
    "text": "What are some examples?\n\ngrepl() has boolean perl and fixed arguments, but you’re not really toggling two independent settings, you’re picking from one of three regular expression engines (the default, the engine used by Perl, and fixed matches). Additionally, the ignore.case argument only applies to two of the strategies.\nggplot2::geom_histogram() has three main strategies for defining the bins: you can supply the number of bins, the width of each bin (the binwidth), or the exact breaks. But it’s very difficult to tell this from the function specification, and there are complex argument dependencies.\ndplyr::left_join() uses an advanced form of this pattern where the different strategies for joining two data frames together are expressed in a mini-DSL provided by dplyr::join_by()."
  },
  {
    "objectID": "strategy-objects.html#how-do-you-use-the-pattern",
    "href": "strategy-objects.html#how-do-you-use-the-pattern",
    "title": "Extract strategies into objects",
    "section": "How do you use the pattern?",
    "text": "How do you use the pattern?\nIn more complicated cases, different strategies will require different arguments, so you’ll need a bit more infrastructure. The basic idea is to build on the options object described in Chapter 11, but instead of providing just one helper function, you’ll provide one function per strategy. This is the way stringr works: you can select a different matching engine by wrapping the pattern in one of regex(), boundary(), coll(), or fixed(). We’ll explore how stringr ended up with design and how you can implement something similar yourself by looking at the base regular expression functions.\nSelecting a pattern engine\nThe basic regular expression functions (grep(), grepl(), sub(), gsub(), regexpr(), gregexpr(), regexec(), and gregexec()) all fixed and perl arguments that allow to select the regular expression engine that’s used:\n\n\nperl = FALSE, fixed = FALSE, the default, uses POSIX 1003.2 extended regular expressions.\n\nperl = TRUE, fixed = FALSE uses Perl-style regular expressions.\n\nperl = FALSE, fixed = TRUE uses fixed matching.\n\nperl = TRUE, fixed = TRUE is an error.\n\nYou could make this choice more clear by using an enumeration (Chapter 10) maybe something like engine = c(\"POSIX\", \"perl\", \"fixed\"). That might look something like this:\n\ngrepl(pattern, string, engine = \"regex\")\ngrepl(pattern, string, engine = \"fixed\")\ngrepl(pattern, string, engine = \"perl\")\n\nBut there’s an additional argument that throws a spanner in the works: ignore.case = TRUE only works with two of the three engines: POSIX and perl. Additionally, it’s a bit unforunate that the engine argument, which is likely to come later in the call, affects the pattern, the first argument. That means you have to read the call until you see the engine argument before you can understand precisely what the pattern means.\nAn alternative approach, as used by stringr, is to provide some helper functions that encode the engine as an attribute of the pattern:\n\ngrepl(pattern, regex(string))\ngrepl(pattern, fixed(string))\ngrepl(pattern, perl(string))\n\nAnd because these are separate functions, they can take different arguments:\n\nregex &lt;- function(pattern, ignore.case = FALSE) {}\nperl &lt;- function(pattern, ignore.case = FALSE) {}\nfixed &lt;- function(pattern) {}\n\nThis gives a very flexible interface which is particularly nice in stringr because it means there’s an easy way to support boundary matching, which doesn’t even take a pattern:\n\nlibrary(stringr)\nstr_view(\"This is a sentence.\", boundary(\"word\"))\n#&gt; [1] │ &lt;This&gt; &lt;is&gt; &lt;a&gt; &lt;sentence&gt;.\nstr_view(\"This is a sentence.\", boundary(\"sentence\"))\n#&gt; [1] │ &lt;This is a sentence.&gt;\n\nImplementation\nLets flesh this interface into an implementation. First we flesh out the pattern engine wrappers. These need to return an object that has the name of engine, the pattern, and any other arguments:\n\nregex &lt;- function(pattern, ignore.case = FALSE) {\n  list(pattern = pattern, engine = \"regex\", ignore.case = ignore.case)\n}\nperl &lt;- function(pattern, ignore.case = FALSE) {\n  list(pattern = pattern, engine = \"perl\", ignore.case = ignore.case)\n}\nfixed &lt;- function(pattern) {\n  list(pattern = pattern, engine = \"fixed\")\n}\n\nThen you could create a new grepl() variant that might look something like this:\n\nmy_grepl &lt;- function(pattern, x, useBytes = FALSE) {\n  switch(pattern$engine, \n    regex = grepl(pattern$pattern, x, ignore.case = pattern$ignore.case, useBytes = useBytes),\n    perl = grepl(pattern$pattern, x, perl = TRUE, ignore.case = pattern$ignore.case, useBytes = useBytes),\n    fixed = grepl(pattern$pattern, x, fixed = TRUE, useBytes = useBytes)\n  )\n}\n\nOr if you wanted to make it more clear how the engines differ, you could pull out a helper function that pulls out the repeated code:\n\nmy_grepl &lt;- function(pattern, x, useBytes = FALSE) {\n  grepl_wrapper &lt;- function(...) {\n    grepl(pattern$pattern, x, ..., useBytes = useBytes)\n  }\n  \n  switch(pattern$engine, \n    regex = grepl_wrapper(ignore.case = pattern$ignore.case),\n    perl = grepl_wrapper(perl = TRUE, ignore.case = pattern$ignore.case),\n    fixed = grepl_wrapper(fixed = TRUE)\n  )\n}\n\nHere I’m just wrapping around the existing grepl() implementation because I don’t want to go into the details of its implementation; for your own code you’d probably inline the implementation.\nI particularly like the switch pattern here and in stringr because it keeps the function calls close together, which makes it easier to keep them in sync. You could also implement the same strategy using if or S7 generic functions, depending on your needs.\nThis is implementation a sketch that gives you the basic ideas. For a real implementation you’d also need to consider:\n\nAre fixed(), perl(), and regex() the right names? Would it be useful to give them a common prefix?\nIt would be better for the engines to return an S7 object instead of a list, so we could provide a print method to make them display more nicely.\n\ngrepl() needs some error checking to ensure that pattern is generated by one of the engines, and probably should have a default path to handle bare character vectors as regular expressions (the current default).\n\nYou can see these detailed worked out in the stringr package if you look at the source code, particularly that of fixed(), type(), opts(), then str_detect()."
  },
  {
    "objectID": "strategy-objects.html#how-do-i-remediate-past-problems",
    "href": "strategy-objects.html#how-do-i-remediate-past-problems",
    "title": "Extract strategies into objects",
    "section": "How do I remediate past problems?",
    "text": "How do I remediate past problems?\nIf you previously used an enum, and now you need a"
  },
  {
    "objectID": "strategy-functions.html#whats-the-problem",
    "href": "strategy-functions.html#whats-the-problem",
    "title": "Three functions in trench coat",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nSometimes a function that implements multiple strategies might be better off as independent functions. Two signs that this might be the case:\n\nYou’re struggling to document how the arguments interact. Maybe you can set a and b and a and c but not b and c.\nThe implementation of your function has a couple of big if branches that share relatively little code.\n\nSplitting one complex multi-strategy function into multiple simpler functions can make maintenance and testing easier, while also improving the user experience. I think of this the three (or more generally \\(n\\)) functions in a trench coat1 problem, because the reality of the separate functions tends to become very obvious in time."
  },
  {
    "objectID": "strategy-functions.html#what-are-some-examples",
    "href": "strategy-functions.html#what-are-some-examples",
    "title": "Three functions in trench coat",
    "section": "What are some examples?",
    "text": "What are some examples?\n\nforcats::fct_lump() chooses between one of three lumping strategies depending on whether you supply just n, just prop, or neither, while supplying both n and prop is an error. Additionally, the ties.method argument only does anything if you supply only n. fct_lump() is hard to understand and document because it’s really three smaller functions.\nDepending on the arguments used library() can load a package, it can list all installed packages, or display the help for a package.\ndiag() is used to both extract the diagonal of a matrix and construct a matrix from a vector of diagonal values. This combination of purposes makes its arguments hard to understand: if x is a matrix, you can names but not nrow and ncol; if x is a vector, you can use nrow and ncol but not names.\nsample() is used to both randomly reorder a vector and generate a random vector of specified length. This function is particularly troublesome because it picks between the two strategies based on the length of the first argument.\nrep() is used to both repeat each element of a vector and to repeat the complete vector. I discuss this more in Chapter 16.\n\nThese functions all also suffer from the problem that the strategies are implicit, not explicit. That’s because they use either the presence or absence of different arguments or the type of an argument pick strategies. This combination tends to produce particularly opaque code."
  },
  {
    "objectID": "strategy-functions.html#how-do-i-identify-the-problem",
    "href": "strategy-functions.html#how-do-i-identify-the-problem",
    "title": "Three functions in trench coat",
    "section": "How do I identify the problem?",
    "text": "How do I identify the problem?\nTypically this problem arises as the scope of your function grows over time, and because the growth tends to be gradual it’s hard to notice exactly when it becomes an issue. One way to splot this problem is notice that your function consists of a big if statement where the branches share very little code. An extreme example of this is the base sample() function. As of R 4.3.0 it looks something like this:\n\nsample &lt;- function(x, size, replace = FALSE, prob = NULL) {\n  if (length(x) == 1L && is.numeric(x) && is.finite(x) && x &gt;= 1) {\n    if (missing(size))\n      size &lt;- x\n    sample.int(x, size, replace, prob)\n  } else {\n    if (missing(size))\n      size &lt;- length(x)\n    x[sample.int(length(x), size, replace, prob)]\n  }\n}\n\nYou can see that there are two branches that share very little code, and each branch uses a different default value for size. This suggests it might be better to have two functions:\n\nsample_vec &lt;- function(x, size = length(x), replace = FALSE, prob = NULL) {\n  # check_vector(x)\n  # check_number_whole(size)\n  \n  x[sample.int(length(x), size, replace, prob)]\n}\n\nsample_int &lt;- function(x, size = x, replace = FALSE, prob = NULL) {\n  # check_number_whole(x)\n  # check_number_whole(size)\n  \n  x[sample.int(length(x), size, replace, prob)]\n}\n\nIn other cases you might spot the problem because you’re having trouble explaining the arguments in the documentation. If it feels like"
  },
  {
    "objectID": "strategy-functions.html#how-do-i-remediate-past-mistakes",
    "href": "strategy-functions.html#how-do-i-remediate-past-mistakes",
    "title": "Three functions in trench coat",
    "section": "How do I remediate past mistakes?",
    "text": "How do I remediate past mistakes?\nRemediating past mistakes is straightforward: define, document, and export one function for each strategy. Then rewrite the original function to use those strategies, deprecating that entire function if desired. For example, this is what fct_lump() looked like after we realised it was really the combination of three simpler functions:\n\nfct_lump &lt;- function(f,\n                     n,\n                     prop,\n                     w = NULL,\n                     other_level = \"Other\",\n                     ties.method = c(\"min\", \"average\", \"first\", \"last\", \"random\", \"max\")) {\n    if (missing(n) && missing(prop)) {\n      fct_lump_lowfreq(f, w = w, other_level = other_level)\n    } else if (missing(prop)) {\n      fct_lump_n(f, n, w = w, other_level = other_level, ties.method = ties.method)\n    } else if (missing(n)) {\n      fct_lump_prop(f, prop, w = w, other_level = other_level)\n    } else {\n      cli::cli_abort(\"Must supply only one of {.arg n} and {.arg prop}.\")\n    }\n}\n\nWe decided to supersede fct_lump() rather than deprecating it, so we kept old function around and working. If we wanted to deprecate it, we’d need to add one deprecation for each branch:\n\nfct_lump &lt;- function(f,\n                     n,\n                     prop,\n                     w = NULL,\n                     other_level = \"Other\",\n                     ties.method = c(\"min\", \"average\", \"first\", \"last\", \"random\", \"max\")) {\n    if (missing(n) && missing(prop)) {\n      lifecycle::deprecate_warn(\"0.5.0\", \"fct_lump()\", \"fct_lump_lowfreq()\")\n      fct_lump_lowfreq(f, w = w, other_level = other_level)\n    } else if (missing(prop)) {\n      lifecycle::deprecate_warn(\"0.5.0\", \"fct_lump()\", \"fct_lump_n()\")\n      fct_lump_n(f, n, w = w, other_level = other_level, ties.method = ties.method)\n    } else if (missing(n)) {\n      lifecycle::deprecate_warn(\"0.5.0\", \"fct_lump()\", \"fct_lump_prop()\")\n      fct_lump_prop(f, prop, w = w, other_level = other_level)\n    } else {\n      cli::cli_abort(\"Must supply only one of {.arg n} and {.arg prop}.\")\n    }\n}"
  },
  {
    "objectID": "strategy-functions.html#footnotes",
    "href": "strategy-functions.html#footnotes",
    "title": "Three functions in trench coat",
    "section": "",
    "text": "https://tvtropes.org/pmwiki/pmwiki.php/Main/TotemPoleTrench↩︎"
  },
  {
    "objectID": "cs-rep.html#what-does-rep-do",
    "href": "cs-rep.html#what-does-rep-do",
    "title": "Case study: rep()",
    "section": "What does rep() do?",
    "text": "What does rep() do?\nrep() is an extremely useful base R function that repeats a vector x in various ways. It takes a vector of data in x and has arguments (times, each, and length.out1) that control how x is repeated. Let’s start by exploring the basics:\n\nx &lt;- c(1, 2, 4)\n\nrep(x, times = 3)\n#&gt; [1] 1 2 4 1 2 4 1 2 4\nrep(x, length.out = 10)\n#&gt;  [1] 1 2 4 1 2 4 1 2 4 1\n\ntimes and length.out replicate the vector in the same way, but length.out allows you to specify a non-integer number of replications.\nThe each argument repeats individual components of the vector rather than the whole vector:\n\nrep(x, each = 3)\n#&gt; [1] 1 1 1 2 2 2 4 4 4\n\nAnd you can combine that with times:\n\nrep(x, each = 3, times = 2)\n#&gt;  [1] 1 1 1 2 2 2 4 4 4 1 1 1 2 2 2 4 4 4\n\nIf you supply a vector to times it works a similar way to each, repeating each component the specified number of times:\n\nrep(x, times = x)\n#&gt; [1] 1 2 2 4 4 4 4"
  },
  {
    "objectID": "cs-rep.html#what-makes-this-function-hard-to-understand",
    "href": "cs-rep.html#what-makes-this-function-hard-to-understand",
    "title": "Case study: rep()",
    "section": "What makes this function hard to understand?",
    "text": "What makes this function hard to understand?\n\n\ntimes and length.out both control the same underlying variable in different ways, and if you set them both then length.out silently wins:\n\nrep(1:3, times = 2, length.out = 3)\n#&gt; [1] 1 2 3\n\n\n\ntimes and each are usually independent:\n\nrep(1:3, times = 2, each = 2)\n#&gt;  [1] 1 1 2 2 3 3 1 1 2 2 3 3\n\nBut if you specify a vector for times you can’t use each.\n\nrep(1:3, times = c(2, 2, 2), each = 2)\n#&gt; Error in rep(1:3, times = c(2, 2, 2), each = 2): invalid 'times' argument\n\n\n\nI think using times with a vector is confusing because it switches from replicating the whole vector to replicating individual values, like each usually does.\n\nrep(1:3, each = 2)\n#&gt; [1] 1 1 2 2 3 3\nrep(1:3, times = 2)\n#&gt; [1] 1 2 3 1 2 3\nrep(1:3, times = c(2, 2, 2))\n#&gt; [1] 1 1 2 2 3 3"
  },
  {
    "objectID": "cs-rep.html#how-might-we-improve-the-situation",
    "href": "cs-rep.html#how-might-we-improve-the-situation",
    "title": "Case study: rep()",
    "section": "How might we improve the situation?",
    "text": "How might we improve the situation?\nI think these problems have the same underlying cause: rep() is trying to do too much in a single function. rep() is really two functions in a trench coat (Chapter 15) and it would be better served by a pair of functions, one which replicates element-by-element, and one which replicates the whole vector.\nThe following sections consider how we might do so, starting with what we should call the functions, then what arguments they’ll need, then what an implementation might look like, and then considering the downsides of this approach.\nFunction names\nTo create the new functions, we need to first come up with names: I like rep_each() and rep_full(). rep_each() was a fairly easy name to come up with because it’ll repeating each element. rep_full() was a little harder and took a few iterations: I like that full has the same number of letters as each, which makes the two functions look like they belong together.\nSome other possibilities I considered:\n\n\nrep_each() + rep_every(): each and every form a natural pair, but to me at least, repeating “every” element doesn’t feel very different to repeating each element.\n\nrep_element() and rep_whole(): I like how these capture the differences precisely, but they are maybe too long for such commonly used functions.\nArguments\nNext, we need to think about their arguments. They both will start with x, the vector to repeat. Then their arguments differ:\n\n\nrep_each() needs an argument that specifies the number of times to repeat each element, which can either be a single number, or a vector the same length as x.\n\nrep_full() has two mutually exclusive arguments (Chapter 17), either the number of times to repeat the whole vector or the desired length of the output.\n\nWhat should we call the arguments? We’ve already captured the different replication strategies in the function name, so I think the argument that specifies the number of times to replicate can be the same for both functions, and times seems reasonable.\nWhat about the second argument to rep_full() which specifies the desired length of the output vector? I draw inspiration from rep() which uses length.out. But I think it’s obvious that the argument controls the output length, so length is adequate.\nImplementation\nWe can combine these specifications with a simple implementation that uses the existing rep function.2\n\nrep_full &lt;- function(x, times, length) {\n  rlang::check_exclusive(times, length)\n  \n  if (!missing(length)) {\n    rep(x, length.out = length)\n  } else {\n    rep(x, times = times)\n  }\n}\n\nrep_each &lt;- function(x, times) {\n  if (length(times) == 1) {\n    rep(x, each = times)\n  } else if (length(times) == length(x)) {\n    rep(x, times = times)\n  } else {\n    stop('`times` must be length 1 or the same length as `x`')\n  }\n}\n\nWe can quickly check that the functions behave as we expect:\n\nx &lt;- c(1, 2, 4)\n\n# First the common times argument\nrep_each(x, times = 3)\n#&gt; [1] 1 1 1 2 2 2 4 4 4\nrep_full(x, times = 3)\n#&gt; [1] 1 2 4 1 2 4 1 2 4\n\n# Then a vector times argument to rep_each:\nrep_each(x, times = x)\n#&gt; [1] 1 2 2 4 4 4 4\n\n# Then the length argumetn to rep_full\nrep_full(x, length = 5)\n#&gt; [1] 1 2 4 1 2\n\nDownsides\nOne downside of this approach is if you want to both replicate each component and the entire vector, you have to use two function calls, which you might expect to be more verbose. However, I don’t think this is a terribly common use case, and if we use our usual call naming conventions, then the new call is the same length:\n\nrep(x, each = 2, times = 3)\n#&gt;  [1] 1 1 2 2 4 4 1 1 2 2 4 4 1 1 2 2 4 4\nrep_full(rep_each(x, 2), 3)\n#&gt;  [1] 1 1 2 2 4 4 1 1 2 2 4 4 1 1 2 2 4 4\n\nAnd it’s only slightly longer if you use the pipe, which is maybe slightly more readable:\n\nx |&gt; rep_each(2) |&gt; rep_full(3)\n#&gt;  [1] 1 1 2 2 4 4 1 1 2 2 4 4 1 1 2 2 4 4\n\n\n\n\n\n\n\nNote that this implementation lacks any input checking so invalid inputs might work, warn, or throw an unhelpful error. For example, since we’re not checking that times and length argument to rep_full() are single integers, the following calls give suboptimal results:\n\nrep_full(1:3, 1:3)\n#&gt; [1] 1 2 2 3 3 3\nrep_full(1:3, \"x\")\n#&gt; Warning in rep_full(1:3, \"x\"): NAs introduced by coercion\n#&gt; Error in rep(x, times = times): invalid 'times' argument\n\nWe’ll come back to input checking later in the book."
  },
  {
    "objectID": "cs-rep.html#footnotes",
    "href": "cs-rep.html#footnotes",
    "title": "Case study: rep()",
    "section": "",
    "text": "Note that the function specification is rep(x, ...), and times, each, and length.out do not appear explicitly. You have to read the documentation to discover these arguments.↩︎\nIn real code I’d want to turn these into explicit unit tests so we can run them repeatedly as we make changes.↩︎"
  },
  {
    "objectID": "implicit-strategies.html#whats-the-pattern",
    "href": "implicit-strategies.html#whats-the-pattern",
    "title": "Implicit strategies",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nThere are two implicit strategies that are sometimes useful. I call them implicit because you don’t select them explicitly with a single argument, but instead select between them based on the presence and absence of different arguments. As you might guess, this can make for a confusing interface, but it is occassionaly the best option.\n\nWith mutually exclusive arguments you select between two strategies based on whether you supply argument a or argument b.\nWith compound objects you select between two strategies based on whether you supply one complex object (e.g. a data frame) or multiple simple objects (e.g. vectors). I think the most compelling reason to use this pattern is when another function might be called directly by a user (who will supply individual arguments) or with the output from another function (which needs to pour into a single argument).\n\nThe main challenge with using these pattern is that you can’t make them clear from the function signature alone, so you need to carefully document and check inputs yourself. They are also likely to be surprising to the user as they are relatively rare patterns. So before using either of these techniques you should try using an explicit strategy via an enum (Chapter 10), using separate functions (Chapter 15), or using strategy objects (Chapter 14). Chapter 18 explores these options from the perspective of rvest::read_html()."
  },
  {
    "objectID": "implicit-strategies.html#what-are-some-examples",
    "href": "implicit-strategies.html#what-are-some-examples",
    "title": "Implicit strategies",
    "section": "What are some examples?",
    "text": "What are some examples?\nMutually exclusive arguments\n\ncutree() is an example where I think mutually exclusive arguments shine: it’s so simple\nread.table() allows you to supply data either with a path to a file, or inline as text. If you supply both, path wins.\nIn ggplot2::scale_x_date() and friends you can specify the breaks and labels either with breaks and labels (like all other scale functions) or with date_breaks and date_labels. If you set both values in a pair, the date_ version wins.\nforcats::fct_other() allows you to either keep or drop specified factor values. If supply neither, or both, you get an error.\ndplyr::relocate() has optional .before and .after arguments.\nCompound objects\n\n\nFor example, it seems reasonable that you should be able to feed the output of str_locate() directly into str_sub():\n\nlibrary(stringr)\n\nx &lt;- c(\"aaaaab\", \"aaab\", \"ccccb\")\nloc &lt;- str_locate(x, \"a+b\")\n\nstr_sub(x, loc)\n#&gt; [1] \"aaaaab\" \"aaab\"   NA\n\nBut equally, it’s nice to be able to supply individual start and end values when calling it directly:\n\nstr_sub(\"Hadley\", start = 2, end = 4)\n#&gt; [1] \"adl\"\n\nSo str_sub() allows either individual vectors supplied to start and end, or a two-column matrix supplied to start.\n\n\noptions(list(a = 1, b = 2)) is equivalent to options(a = 1, b = 2). This is half of very useful pattern. The other half of that pattern is that options() returns the previous value of any options that you set. That means you can do old &lt;- options(…); options(old) to temporarily set options with in a function.\nwithr::local_options() and withr::local_envvar() work similarly: you can either supply a single list of values, or individually named values. But they do it with different arguments.\n\n\nAnother place that this pattern crops up is in dplyr::bind_rows(). When binding rows together, it’s equally useful to bind a few named data frames as it is to bind a list of data frames that come from map or similar. In base R you need to know about do.call(rbind, mylist) which is a relatively sophisticated pattern. So in dplyr we tried to make bind_rows() automatically figure out if you were in situation one or two. Unfortunately, it turns out to be really hard to tell which of the situations you are in, so dplyr implemented heuristics that work most of the time, but occasionally it fails in surprising ways.\nNow we have generally steered away from interfaces that try to automatically “unsplice” their inputs and instead require that you use !!! to explicitly unsplice. This is has some advantages and disadvantages: it’s an interface that’s becoming increasingly common in the tidyverse (and we have a good convention for documenting it with the &lt;dynamic-dots&gt; tag), but it’s still relatively rare and is an advanced technique that we don’t expect everyone to learn. That’s why for this important case, we also have purrr::list_cbind().\nBut it means that functions like purrr::hoist(), forcats::fct_cross(), and rvest::html_form() which are less commonly given lists have a clearly documented escape hatch that doesn’t require another different function. (And of course if you understand the do.call pattern you can still use that too)."
  },
  {
    "objectID": "implicit-strategies.html#how-do-you-use-this-pattern",
    "href": "implicit-strategies.html#how-do-you-use-this-pattern",
    "title": "Implicit strategies",
    "section": "How do you use this pattern?",
    "text": "How do you use this pattern?\nMutually exclusive arguments\nIf a function needs to have mutually exclusive arguments (i.e. you must supply only one of theme) make sure you check that only one is supplied in order to give a clear error message. Avoid implementing some precedence order where if both a and b are supplied, b silently wins. The easiest way to do this is to use rlang::check_exclusive().\n(In the case of required args, you might want to consider putting them after …. This violations Chapter 8, but forces the user to name the arguments which will make the code easier to read)\nIf you must pick one of the two mutually exclusive arguments, make their defaults empty. Otherwise, if they’re optional, give them NULL arguments.\n\nfct_drop &lt;- function(f, drop, keep) {\n  rlang::check_exclusive(drop, keep)\n}\n\nfct_drop(factor())\n#&gt; Error in `fct_drop()`:\n#&gt; ! One of `drop` or `keep` must be supplied.\n\nfct_drop(factor(), keep = \"a\", drop = \"b\")\n#&gt; Error in `fct_drop()`:\n#&gt; ! Exactly one of `drop` or `keep` must be supplied.\n\n(If the arguments are optional, you’ll need .require = FALSE until https://github.com/r-lib/rlang/issues/1647)\n\n\n\n\n\n\nWith base R\n\n\n\n\n\nIf you don’t want to use rlang, you implement yourself with xor() and missing():\n\nfct_drop &lt;- function(f, drop, keep) {\n  if (!xor(missing(keep), missing(drop))) {\n    stop(\"Exactly one of `keep` and `drop` must be supplied\")\n  }  \n}\nfct_drop(factor())\n\nfct_drop(factor(), keep = \"a\", drop = \"b\")\n\n\n\n\nIn the documentation, document the pair of arguments together, and make it clear that only one of the pair can be supplied:\n\n#' @param keep,drop Pick one of `keep` and `drop`:\n#'   * `keep` will preserve listed levels, replacing all others with \n#'     `other_level`.\n#'   * `drop` will replace listed levels with `other_level`, keeping all\n#'     as is.\n\nCompound arguments\nTo implement in your own functions, you should branch on the type of the first argument and then check that the others aren’t supplied.\n\nstr_sub &lt;- function(string, start, end) {\n  if (is.matrix(start)) {\n    if (!missing(end)) {\n      abort(\"`end` must be missing when `start` is a matrix\")\n    }\n    if (ncol(start) != 2) {\n      abort(\"Matrix `start` must have exactly two columns\")\n    }\n    stri_sub(string, from = start[, 1], to = start[, 2])\n  } else {\n    stri_sub(string, from = start, to = end)\n  }\n}\n\nAnd make it clear in the documentation:\n\n#' @param start,end Integer vectors giving the `start` (default: first)\n#'   and `end` (default: last) positions, inclusively. \n#'   \n#'   Alternatively, you pass a two-column matrix to `start`, i.e. \n#'   `str_sub(x, start, end)` is equivalent to \n#'   `str_sub(x, cbind(start, end))`\n\n(If you look at string::str_sub() you’ll notice that start and end do have defaults; I think this is a mistake because start and end are important enough that the user should always be forced to supply them.)"
  },
  {
    "objectID": "cs-rvest.html#what-does-the-function-do",
    "href": "cs-rvest.html#what-does-the-function-do",
    "title": "Case study: html_element()",
    "section": "What does the function do?",
    "text": "What does the function do?\nrvest::html_element() is used to extract matching HTML elements/nodes from a web page. You can select nodes using one of two languages: CSS selectors or XPath expressions. These are both mini-languages for describing how to find the node you want. You can think of them like regular expressions, but instead of being design to find patterns in strings, they are designed to find patterns in trees (since HTML nodes form a tree).\nInteresting case because CSS selectors are much much simpler and likely to be used the majority of the time. XPath is a much richer and more powerful language, but most of the time that complexity is not required and just adds unneeded overhead. (One interesting wrinkle is that CSS selectors actually use XPath behind the hood because they are transformed using the selectr package by Simon Potter).\nhtml_element() implements these two strategies using mutually exclusive css and xpath arguments.\nOther approaches:\n\nhtml_element(x, selector, type = c(\"css\", \"xpath\"))\n\nhtml_element(x, css``(pattern``)) vs html_element(x, xpath(pattern))\n\n\nhtml_element_css(x, pattern), html_element_xpath(x,``pattern``)\n\n\n\n\n\n\n\n\nCommon case\nRare case\n\n\n\nx |&gt; html_element(\"sel\")\nx |&gt; html_element(\"sel\", type = \"xpath\")\n\n\nx |&gt; html_element(\"sel\")\nx |&gt; html_element(xpath = \"sel\")\n\n\nx |&gt; html_element(\"sel\")\nx |&gt; html_element(xpath(\"sel\"))\n\n\nx |&gt; html_element(\"sel\")\nx |&gt; html_element_xpath(\"sel\")"
  },
  {
    "objectID": "def-magical.html#whats-the-problem",
    "href": "def-magical.html#whats-the-problem",
    "title": "Avoid magical defaults",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nIf a function behaves differently when the default value is supplied explicitly, we say it has a magical default. Magical defaults are best avoided because they make it harder to interpret the function specification."
  },
  {
    "objectID": "def-magical.html#what-are-some-examples",
    "href": "def-magical.html#what-are-some-examples",
    "title": "Avoid magical defaults",
    "section": "What are some examples?",
    "text": "What are some examples?\n\n\nIn data.frame(), the default argument for row.names is NULL, but if you supply it directly you get a different result:\n\nargs(data.frame)\n#&gt; function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, \n#&gt;     fix.empty.names = TRUE, stringsAsFactors = FALSE) \n#&gt; NULL\n\nx &lt;- setNames(nm = letters[1:2])\nx\n#&gt;   a   b \n#&gt; \"a\" \"b\"\n\ndata.frame(x)\n#&gt;   x\n#&gt; a a\n#&gt; b b\n\ndata.frame(x, row.names = NULL)\n#&gt;   x\n#&gt; 1 a\n#&gt; 2 b\n\n\n\nIn hist(), the default value of xlim is range(breaks), and the default value for breaks is \"Sturges\". range(\"Sturges\") returns c(\"Sturges\", \"Sturges\") which doesn’t work when supplied explicitly:\n\nargs(hist.default)\n#&gt; function (x, breaks = \"Sturges\", freq = NULL, probability = !freq, \n#&gt;     include.lowest = TRUE, right = TRUE, fuzz = 1e-07, density = NULL, \n#&gt;     angle = 45, col = \"lightgray\", border = NULL, main = paste(\"Histogram of\", \n#&gt;         xname), xlim = range(breaks), ylim = NULL, xlab = xname, \n#&gt;     ylab, axes = TRUE, plot = TRUE, labels = FALSE, nclass = NULL, \n#&gt;     warn.unused = TRUE, ...) \n#&gt; NULL\n\nhist(1:10, xlim = c(\"Sturges\", \"Sturges\"))\n#&gt; Error in plot.window(xlim, ylim, \"\", ...): invalid 'xlim' value\n\n\n\nreadr::read_csv() has progress = show_progress(), but until version 1.3.1, show_progress() was not exported from the package. That means if you attempted to run it yourself, you’d see an error message:\n\nshow_progress()\n#&gt; Error in show_progress(): could not find function \"show_progress\""
  },
  {
    "objectID": "def-magical.html#what-are-the-exceptions",
    "href": "def-magical.html#what-are-the-exceptions",
    "title": "Avoid magical defaults",
    "section": "What are the exceptions?",
    "text": "What are the exceptions?\nIt’s ok to use this behaviour when you want the default value of one argument to be the same as another. For example, take rlang::set_names(), which allows you to create a named vector from two inputs:\n\nlibrary(rlang)\nargs(set_names)\n#&gt; function (x, nm = x, ...) \n#&gt; NULL\n\nset_names(1:3, letters[1:3])\n#&gt; a b c \n#&gt; 1 2 3\n\nThe default value for the names is the vector itself. This provides a convenient shortcut for naming a vector with itself:\n\nset_names(letters[1:3])\n#&gt;   a   b   c \n#&gt; \"a\" \"b\" \"c\"\n\nYou can see this same technique in merge(), where all.x and all.y default to the same value as all, and in factor() where labels defaults to the same value as levels.\nIf you use this technique, make sure that you never use the value of an argument that comes later in the argument list. For example, in file.copy() overwrite defaults to the same value as recursive, but the recursive argument is defined after overwrite:\n\nargs(file.copy)\n#&gt; function (from, to, overwrite = recursive, recursive = FALSE, \n#&gt;     copy.mode = TRUE, copy.date = FALSE) \n#&gt; NULL\n\nThis makes the defaults arguments harder to understand because you can’t just read from left-to-right."
  },
  {
    "objectID": "def-magical.html#what-causes-the-problem",
    "href": "def-magical.html#what-causes-the-problem",
    "title": "Avoid magical defaults",
    "section": "What causes the problem?",
    "text": "What causes the problem?\nThis problem is generally easy to avoid for new functions:\n\n\nDon’t use default values that depend on variables defined inside the function. The default values of function arguments are lazily evaluated in the environment of the function when they are first used, as described in Advanced R. Here’s a simple example:\n\nf1 &lt;- function(x = y) {\n  y &lt;- 2\n  x\n}\n\ny &lt;- 1\nf1()\n#&gt; [1] 2\nf1(y)\n#&gt; [1] 1\n\nWhen x takes the value y from its default, it’s evaluated inside the function, yielding 1. When y is supplied explicitly, it is evaluated in the caller environment, yielding 2.\n\n\nDon’t use missing()[^def-magical-1].\n\nf2 &lt;- function(x = 1) {\n  if (missing(x)) {\n    2\n  } else {\n    x\n  }\n}\n\nf2()\n#&gt; [1] 2\nf2(1)\n#&gt; [1] 1\n\n\nDon’t use unexported functions. In packages, it’s easy to use a non-exported function without thinking about it. This function is available to you, the package author, but not the user of the package, which makes it harder for them to understand how a package works."
  },
  {
    "objectID": "def-magical.html#how-do-i-remediate-the-problem",
    "href": "def-magical.html#how-do-i-remediate-the-problem",
    "title": "Avoid magical defaults",
    "section": "How do I remediate the problem?",
    "text": "How do I remediate the problem?\nIf you have a made a mistake in an older function you can remediate it by using a NULL default, as described in Chapter 9). If the problem is caused by an unexported function, you can also choose to document and export it. Remediating this problem shouldn’t break existing code, because it expands the function interface: all previous code will continue to work, and the function will also work if the argument is passed NULL input (which probably didn’t previously)."
  },
  {
    "objectID": "def-inform.html#whats-the-pattern",
    "href": "def-inform.html#whats-the-pattern",
    "title": "Explain important defaults",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nIf a default value is important, and the computation is non-trivial, inform the user what value was used. This is particularly important when the default value is an educated guess, and you want the user to change it. It is also important when descriptor arguments (Chapter 6)) have defaults."
  },
  {
    "objectID": "def-inform.html#what-are-some-examples",
    "href": "def-inform.html#what-are-some-examples",
    "title": "Explain important defaults",
    "section": "What are some examples?",
    "text": "What are some examples?\n\n\ndplyr::left_join() and friends automatically compute the variables to join by as the variables that occur in both x and y (this is called a natural join in SQL). This is convenient, but it’s a heuristic so doesn’t always work.\n\nlibrary(nycflights13)\nlibrary(dplyr)\n\n# Correct    \nout &lt;- left_join(flights, airlines)\n#&gt; Joining with `by = join_by(carrier)`\n\n# Incorrect\nout &lt;- left_join(flights, planes)\n#&gt; Joining with `by = join_by(year, tailnum)`\n\n# Error\nout &lt;- left_join(flights, airports)\n#&gt; Error in `left_join()`:\n#&gt; ! `by` must be supplied when `x` and `y` have no common variables.\n#&gt; ℹ Use `cross_join()` to perform a cross-join.\n\n\n\nreadr::read_csv() reads a csv file into a data frame. Because csv files don’t store the type of each variable, readr must guess the types. In order to be fast, read_csv() uses some heuristics, so it might guess wrong. Or maybe guesses correctly today, but when your automated script runs in two months time when the data format has changed, it might guess incorrectly and give weird downstream errors. For this reason, read_csv() prints the column specification in a way that you can copy-and-paste into your code.\n\nlibrary(readr)\nmtcars &lt;- read_csv(readr_example(\"mtcars.csv\"))\n#&gt; Rows: 32 Columns: 11\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (11): mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nIn ggplot2::geom_histogram(), the binwidth is an important parameter that you should always experiment with. This suggests it should be a required argument, but it’s hard to know what values to try until you’ve seen a plot. For this reason, ggplot2 provides a suboptimal default of 30 bins: this gets you started, and then a message tells you how to modify.\n\nlibrary(ggplot2)\nggplot(diamonds, aes(carat)) + geom_histogram()\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nWhen installing packages, install.packages() informs of the value of the lib argument, which defaults to .libPath()[[1]]:\n\ninstall.packages(\"forcats\")\n# Installing package into ‘/Users/hadley/R’\n# (as ‘lib’ is unspecified)\n\nThis, however, is not terribly important (most people only use one library), it’s easy to ignore this amongst the other output, and the message doesn’t refer to the mechanism that controls the default (.libPaths())."
  },
  {
    "objectID": "def-inform.html#why-is-it-important",
    "href": "def-inform.html#why-is-it-important",
    "title": "Explain important defaults",
    "section": "Why is it important?",
    "text": "Why is it important?\n\nThere are two ways to fire a machine gun in the dark. You can find out exactly where your target is (range, elevation, and azimuth). You can determine the environmental conditions (temperature, humidity, air pressure, wind, and so on). You can determine the precise specifications of the cartridges and bullets you are using, and their interactions with the actual gun you are firing. You can then use tables or a firing computer to calculate the exact bearing and elevation of the barrel. If everything works exactly as specified, your tables are correct, and the environment doesn’t change, your bullets should land close to their target.\nOr you could use tracer bullets.\nTracer bullets are loaded at intervals on the ammo belt alongside regular ammunition. When they’re fired, their phosphorus ignites and leaves a pyrotechnic trail from the gun to whatever they hit. If the tracers are hitting the target, then so are the regular bullets.\n— The Pragmatic Programmer\n\nI think this is a valuable pattern because it helps balance two tensions in function design:\n\nForcing the function user to really think about what they want to do.\nTrying to be helpful, so the user of function can achieve their goal as quickly as possible.\n\nOften your thoughts about a problem will be aided by a first attempt, even if that attempt is wrong. Helps facilitate iteration: you don’t sit down and contemplate for an hour and then write one perfectly formed line of R code. You take a stab at it, look at the result, and then tweak.\nTaking a default that the user really should carefully think about and make a decision on, and turning it into a heurstic or educated guess, and reporting the value, is like a tracer bullet.\nThe counterpoint to this pattern is that people don’t read repeated output. For example, do you know how to cite R in a paper? It’s mentioned every time that you start R. Human brains are extremely good at filtering out unchanging signals, which means that you must use this technique with caution. If every argument tells you the default it uses, it’s effectively the same as doing nothing: the most important signals will get buried in the noise. This is why you’ll see the technique used in only a handful of places in the tidyverse."
  },
  {
    "objectID": "def-inform.html#how-can-i-use-it",
    "href": "def-inform.html#how-can-i-use-it",
    "title": "Explain important defaults",
    "section": "How can I use it?",
    "text": "How can I use it?\nTo use this message you need to generate a message from the computation of the default value. The easiest way to do this to write a small helper function. It should compute the default value given some inputs and generate a message() that gives the code that you could copy and paste into the function call.\nTake the dplyr join functions, for example. They use a function like this:\n\ncommon_by &lt;- function(x, y) {\n  common &lt;- intersect(names(x), names(y))\n  if (length(common) == 0) {\n    stop(\"Must specify `by` when no common variables in `x` and `y`\", call. = FALSE)\n  }\n  \n  message(\"Computing common variables: `by = \", rlang::expr_text(common), \"`\")\n  common\n}\n\ncommon_by(data.frame(x = 1), data.frame(x = 1))\n#&gt; Computing common variables: `by = \"x\"`\n#&gt; [1] \"x\"\ncommon_by(flights, planes)\n#&gt; Computing common variables: `by = c(\"year\", \"tailnum\")`\n#&gt; [1] \"year\"    \"tailnum\"\n\nThe technique you use to generate the code will vary from function to function. rlang::expr_text() is useful here because it automatically creates the code you’d use to build the character vector.\nTo avoid creating a magical default (Chapter 19), either export and document the function, or use one of the techniques in Chapter 9:\n\nleft_join &lt;- function(x, y, by = NULL) {\n  by &lt;- by %||% common_by(x, y)\n}"
  },
  {
    "objectID": "def-user.html#whats-the-pattern",
    "href": "def-user.html#whats-the-pattern",
    "title": "User settable defaults",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nIt’s sometimes useful to give the user control over default values, so that they can set once per session or once for every session in their .Rprofile. To do so, use getOption() in the default value.\nNote that this pattern should general only be used to control the side-effects of a function, not its compute value. The two primary uses are for controlling the appearance of output, particularly in print() methods, and for setting default values in generated templates.\nRelated patterns:\n\nIf a global option affects the results of the computation (not just its side-effects), you have an example of Chapter 5."
  },
  {
    "objectID": "def-user.html#what-are-some-examples",
    "href": "def-user.html#what-are-some-examples",
    "title": "User settable defaults",
    "section": "What are some examples?",
    "text": "What are some examples?"
  },
  {
    "objectID": "def-user.html#why-is-it-important",
    "href": "def-user.html#why-is-it-important",
    "title": "User settable defaults",
    "section": "Why is it important?",
    "text": "Why is it important?"
  },
  {
    "objectID": "def-user.html#what-are-the-exceptions",
    "href": "def-user.html#what-are-the-exceptions",
    "title": "User settable defaults",
    "section": "What are the exceptions?",
    "text": "What are the exceptions?"
  },
  {
    "objectID": "def-user.html#how-do-i-use-it",
    "href": "def-user.html#how-do-i-use-it",
    "title": "User settable defaults",
    "section": "How do I use it?",
    "text": "How do I use it?"
  },
  {
    "objectID": "dots-data.html#whats-the-problem",
    "href": "dots-data.html#whats-the-problem",
    "title": "Making data with …",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nA number of functions take ... to save the user from having to create a vector themselves:"
  },
  {
    "objectID": "dots-data.html#what-are-some-examples",
    "href": "dots-data.html#what-are-some-examples",
    "title": "Making data with …",
    "section": "What are some examples?",
    "text": "What are some examples?\n\nsum(c(1, 1, 1))\n#&gt; [1] 3\n# can be shortened to:\nsum(1, 1, 1)\n#&gt; [1] 3\n\nf &lt;- factor(c(\"a\", \"b\", \"c\", \"d\"), levels = c(\"b\", \"c\", \"d\", \"a\"))\nf\n#&gt; [1] a b c d\n#&gt; Levels: b c d a\nfct_relevel(f, c(\"b\", \"a\"))\n#&gt; [1] a b c d\n#&gt; Levels: b a c d\n# can be shortened to:\nfct_relevel(f, \"b\", \"a\")\n#&gt; [1] a b c d\n#&gt; Levels: b a c d\n\n\nmapply()"
  },
  {
    "objectID": "dots-data.html#why-is-it-important",
    "href": "dots-data.html#why-is-it-important",
    "title": "Making data with …",
    "section": "Why is it important?",
    "text": "Why is it important?\nIn general, I think it is best to avoid using ... for this purpose because it has a relatively small benefit, only reducing typing by three letters c(), but has a number of costs:\n\n\nIt can give the misleading impression that other functions in the same family work the same way. For example, if you’re internalised how sum() works, you might predict that mean() works the same way, but it does not:\n\nmean(c(1, 2, 3))\n#&gt; [1] 2\nmean(1, 2, 3)\n#&gt; [1] 1\n\n(See Chapter Chapter 8 to learn why this doesn’t give an error message.)\n\n\nIt makes it harder to adapt the function for new uses. For example, fct_relevel() can also be called with a function:\n\nfct_relevel(f, sort)\n#&gt; [1] a b c d\n#&gt; Levels: a b c d\n\nIf fct_relevel() took its input as a single vector, you could easily extend it to also work with functions:\n\nfct_relevel &lt;- function(f, x) {\n  if (is.function(x)) {\n    x &lt;- f(levels(x))\n  }\n}\n\nHowever, because fct_relevel() uses dots, the implementation needs to be more complicated:\n\nfct_relevel &lt;- function(f, ...) {\n  if (dots_n(...) == 1L && is.function(..1)) {\n    levels &lt;- fun(levels(x))\n  } else {\n    levels &lt;- c(...)\n  }\n}"
  },
  {
    "objectID": "dots-data.html#what-are-the-exceptions",
    "href": "dots-data.html#what-are-the-exceptions",
    "title": "Making data with …",
    "section": "What are the exceptions?",
    "text": "What are the exceptions?\nNote that in all the examples above, the ... are used to collect a single details argument. It’s ok to use ... to collect data, as in paste(), data.frame(), or list()."
  },
  {
    "objectID": "dots-data.html#how-can-remediate-it",
    "href": "dots-data.html#how-can-remediate-it",
    "title": "Making data with …",
    "section": "How can remediate it?",
    "text": "How can remediate it?\nIf you’ve already published a function where you’ve used ... for this purpose you can change the interface by adding a new argument in front of ..., and then warning if anything ends up in ....\n\nold_foo &lt;- function(x, ...) {\n}\n\nnew_foo &lt;- function(x, y, ...) {\n  if (rlang::dots_n(...) &gt; 0) {\n    warning(\"Use of `...` is now deprecated. Please put all arguments in `y`\")\n    y &lt;- c(y, ...)\n  }\n}\n\nBecause this is a interface change, it should be prominently advertised in packages."
  },
  {
    "objectID": "dots-data.html#how-can-i-protect-myself",
    "href": "dots-data.html#how-can-i-protect-myself",
    "title": "Making data with …",
    "section": "How can I protect myself?",
    "text": "How can I protect myself?\nIf you do feel that the tradeoff is worth it (i.e. it’s an extremely frequently used function and the savings over time will be considerable), you need to take some steps to minimise the downsides.\nThis is easiest if you’re constructing a vector that shouldn’t have names. In this case, you can call rlang::check_dots_unnamed() to ensure that no named arguments have been accidentally passed to .... This protects you against the following undesirable behaviour of sum():\n\nsum(1, 1, 1, na.omit = TRUE)\n#&gt; [1] 4\n\nsafe_sum &lt;- function(..., na.rm = TRUE) {\n  rlang::check_dots_unnamed()\n  sum(c(...), na.rm = na.rm)\n}\nsafe_sum(1, 1, 1, na.omit = TRUE)\n#&gt; Error in `safe_sum()`:\n#&gt; ! Arguments in `...` must be passed by position, not name.\n#&gt; ✖ Problematic argument:\n#&gt; • na.omit = TRUE\n\nIf you want your vector to have names, the problem is harder, and there’s relatively little that you can. You’ll need to ensure that all other arguments get a . prefix (to minimise chances of a mismatch) and then think carefully about how you might detect problems by thinking about the expect type of c(...). As far as I know, there are no general techniques, and you’ll have to think about the problem on a case-by-case basis."
  },
  {
    "objectID": "dots-data.html#selecting-variables",
    "href": "dots-data.html#selecting-variables",
    "title": "Making data with …",
    "section": "Selecting variables",
    "text": "Selecting variables\nA number of funtions in the tidyverse use ... for selecting variables. For example, tidyr::fill() lets you fill in missing values based on the previous row:\n\ndf &lt;- tribble(\n  ~year,  ~month, ~day,\n  2020,  1,       1,\n  NA,    NA,      2,\n  NA,    NA,      3,\n  NA,    2,       1\n)\ndf %&gt;% fill(year, month)\n#&gt; # A tibble: 4 × 3\n#&gt;    year month   day\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2020     1     1\n#&gt; 2  2020     1     2\n#&gt; 3  2020     1     3\n#&gt; 4  2020     2     1\n\nAll functions that work like this include a call to tidyselect::vars_select() that looks something like this:\n\nfind_vars &lt;- function(data, ...) {\n  tidyselect::vars_select(names(data), ...)\n}\n\nfind_vars(df, year, month)\n#&gt;    year   month \n#&gt;  \"year\" \"month\"\n\nI now think that this interface is a mistake because it suffers from the same problem as sum(): we’re using ... to only save a little typing. We can eliminate the use of dots by requiring the user to use c(). (This change also requires explicit quoting and unquoting of vars since we’re no longer using ....)\n\nfoo &lt;- function(data, vars) {\n  tidyselect::vars_select(names(data), !!enquo(vars))\n}\n\nfind_vars(df, c(year, month))\n#&gt;    year   month \n#&gt;  \"year\" \"month\"\n\nIn other words, I believe that better interface to fill() would be:\n\ndf %&gt;% fill(c(year, month))\n\nOther tidyverse functions like dplyr’s scoped verbs and ggplot2::facet_grid() require the user to explicitly quote the input. I now believe that this is also a suboptimal interface because it is more typing (var() is longer than c(), and you must quote even single variables), and arguments that require their inputs to be explicitly quoted are rare in the tidyverse.\n\n# existing interface\ndplyr::mutate_at(mtcars, vars(cyl:vs), mean)\n# what I would create today\ndplyr::mutate_at(mtcars, c(cyl:vs), mean)\n\n# existing interface\nggplot2::facet_grid(rows = vars(drv), cols = vars(vs, am))\n# what I would create today\nggplot2::facet_grid(rows = drv, cols = c(vs, am))\n\nThat said, it is unlikely we will ever change functions, because the benefit is smaller (primarily improved consistency) and the costs are high, as it impossible to switch from an evaluated argument to a quoted argument without breaking backward compatibility in some small percentage of cases."
  },
  {
    "objectID": "dots-prefix.html#whats-the-pattern",
    "href": "dots-prefix.html#whats-the-pattern",
    "title": "Dot prefix",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nWhen using ... to create a data structure, or when passing ... to a user-supplied function, add a . prefix to all named arguments. This reduces (but does not eliminate) the chances of matching an argument at the wrong level. Additionally, you should always provide some mechanism that allows you to escape and use that name if needed.\n\nlibrary(purrr)\n\n(Not important if you ignore names: e.g. cat().)"
  },
  {
    "objectID": "dots-prefix.html#what-are-some-examples",
    "href": "dots-prefix.html#what-are-some-examples",
    "title": "Dot prefix",
    "section": "What are some examples?",
    "text": "What are some examples?\nLook at the arguments to some functions in purrr:\n\nargs(map)\n#&gt; function (.x, .f, ..., .progress = FALSE) \n#&gt; NULL\nargs(reduce)\n#&gt; function (.x, .f, ..., .init, .dir = c(\"forward\", \"backward\")) \n#&gt; NULL\nargs(detect)\n#&gt; function (.x, .f, ..., .dir = c(\"forward\", \"backward\"), .right = NULL, \n#&gt;     .default = NULL) \n#&gt; NULL\n\nNotice that all named arguments start with .. This reduces the chance that you will incorrectly match an argument to map(), rather than to an argument of .f. Obviously it can’t eliminate it.\nEscape mechanism is the anonymous function. Little easier to access in purrr::map() since you can create with ~, which is much less typing than function() {}. For example, imagine you want to…\nExample: https://jennybc.github.io/purrr-tutorial/ls02_map-extraction-advanced.html#list_inside_a_data_frame"
  },
  {
    "objectID": "dots-prefix.html#case-study-dplyr-verbs",
    "href": "dots-prefix.html#case-study-dplyr-verbs",
    "title": "Dot prefix",
    "section": "Case study: dplyr verbs",
    "text": "Case study: dplyr verbs\n\nargs(dplyr::filter)\n#&gt; function (.data, ..., .by = NULL, .preserve = FALSE) \n#&gt; NULL\nargs(dplyr::group_by)\n#&gt; function (.data, ..., .add = FALSE, .drop = group_by_drop_default(.data)) \n#&gt; NULL\n\nEscape hatch is :=.\nOoops:\n\nargs(dplyr::left_join)\n#&gt; function (x, y, by = NULL, copy = FALSE, suffix = c(\".x\", \".y\"), \n#&gt;     ..., keep = NULL) \n#&gt; NULL"
  },
  {
    "objectID": "dots-prefix.html#other-approaches-in-base-r",
    "href": "dots-prefix.html#other-approaches-in-base-r",
    "title": "Dot prefix",
    "section": "Other approaches in base R",
    "text": "Other approaches in base R\nBase R uses two alternative methods: uppercase and _ prefix.\nThe apply family tends to use uppercase function names for the same reason. Unfortunately the functions are a little inconsistent which makes it hard to see this pattern. I think a dot prefix is better because it’s easier to type (you don’t have to hold down the shift-key with one finger).\n\nargs(lapply)\n#&gt; function (X, FUN, ...) \n#&gt; NULL\nargs(sapply)\n#&gt; function (X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE) \n#&gt; NULL\nargs(apply)\n#&gt; function (X, MARGIN, FUN, ..., simplify = TRUE) \n#&gt; NULL\nargs(mapply)\n#&gt; function (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) \n#&gt; NULL\nargs(tapply)\n#&gt; function (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE) \n#&gt; NULL\n\nReduce() and friends avoid the problem altogether by not accepting ..., and requiring that the user creates anonymous functions. But this is verbose, particularly without shortcuts to create functions.\ntransform() goes a step further and uses an non-syntactic variable name.\n\nargs(transform)\n#&gt; function (`_data`, ...) \n#&gt; NULL\n\nUsing a non-syntactic variable names means that it must always be surrounded in `. This means that a user is even less likely to use it that with ., but it increases friction when writing the function. In my opinion, this trade-off is not worth it."
  },
  {
    "objectID": "dots-prefix.html#what-are-the-exceptions",
    "href": "dots-prefix.html#what-are-the-exceptions",
    "title": "Dot prefix",
    "section": "What are the exceptions?",
    "text": "What are the exceptions?\n\n\ntryCatch(): the names give classes so, as long as you don’t create a condition class called expr or finally (which would be weird!) you don’t need to worry about matches."
  },
  {
    "objectID": "dots-inspect.html#whats-the-pattern",
    "href": "dots-inspect.html#whats-the-pattern",
    "title": "Inspect the dots",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nWhenever you use ... in an S3 generic to allow methods to add custom arguments, you should inspect the dots to make sure that every argument is used. You can also use this same approach when passing ... to an overly permissive function."
  },
  {
    "objectID": "dots-inspect.html#what-are-some-examples",
    "href": "dots-inspect.html#what-are-some-examples",
    "title": "Inspect the dots",
    "section": "What are some examples?",
    "text": "What are some examples?\nIf you don’t use this technique it is easy to end up with functions that silently return the incorrect result when argument names are misspelled.\n\n# Misspelled\nweighted.mean(c(1, 0, -1), wt = c(10, 0, 0))\n#&gt; [1] 0\nmean(c(1:9, 100), trim = 0.1)\n#&gt; [1] 5.5\n\n# Correct\nweighted.mean(c(1, 0, -1), w = c(10, 0, 0))\n#&gt; [1] 1\nmean(c(1:9, 100), trim = 0.1)\n#&gt; [1] 5.5"
  },
  {
    "objectID": "dots-inspect.html#how-do-i-do-it",
    "href": "dots-inspect.html#how-do-i-do-it",
    "title": "Inspect the dots",
    "section": "How do I do it?",
    "text": "How do I do it?\nAdd a call to rlang::check_dots_used() in the generic before the call to UseMethod(). This automatically adds an on exit handler, which checks that ever element of ... has been evaluated just prior to the function returnning.\nYou can see this in action by creating a safe wrapper around cut(), which has different arguments for its numeric and date methods.\n\nsafe_cut &lt;- function(x, breaks, ..., right = TRUE) {\n  rlang::check_dots_used()\n  UseMethod(\"safe_cut\")\n}\n\nsafe_cut.numeric &lt;- function(x, breaks, ..., right = TRUE, include.lowest = FALSE) {\n  cut(x, breaks = breaks, right = right, include.lowest = include.lowest)\n}\n\nsafe_cut.Date &lt;- function(x, breaks, ..., right = TRUE, start.on.monday = TRUE) {\n  cut(x, breaks = breaks, right = right, start.on.monday = start.on.monday)\n}\n\nWhat are the limitations?\nAccurately detecting this problem is hard because no one place has all the information needed to tell if an argument is superfluous or not (the precise details are beyond the scope of this text). Instead rlang takes advantage of R’s lazy evaluation and inspects the internal components of ... to see if their evaluation has been forced.\nIf a function is called primarily for its side-effects, the error will occur after the side-effect has happened, making for a confusing result. Here the best we can do is a warning, generated by rlang::check_dots_used(error = function(e) warn(e))\nIf a function captures the components of ... using enquo() or match.call(), you can not use this technique. This also means that if you use check_dots_used(), the method author can not choose to add a quoted argument. I think this is ok because quoting vs. evaluating is part of the interface of the generic, so methods should not change this interface, and it’s fine for the author of the generic to make that decision for all method authors.\nWhat are other uses?\nThis same technique can also be used when you are wrapping other functions. For example, stringr::str_sort() takes ... and passes it on to stringi::stri_opts_collator(). As of March 2019, str_sort() looked like this:\n\nstr_sort &lt;- function(x, decreasing = FALSE, na_last = TRUE, locale = \"en\",  numeric = FALSE, ...) \n{\n    stringi::stri_sort(x, \n      decreasing = decreasing, \n      na_last = na_last, \n      opts_collator = stringi::stri_opts_collator(\n        locale, \n        numeric = numeric, \n        ...\n      )\n    )\n}\n\n\nx &lt;- c(\"x1\", \"x100\", \"x2\")\nstr_sort(x)\n#&gt; [1] \"x1\"   \"x100\" \"x2\"\nstr_sort(x, numeric = TRUE)\n#&gt; [1] \"x1\"   \"x2\"   \"x100\"\n\nThis is wrapper is useful because it decouples str_sort() from the stri_opts_collator() meaning that if stri_opts_collator() gains new arguments users of str_sort() can take advantage of them immediately. But most of the arguments in stri_opts_collator() are sufficiently arcane that they don’t need to be exposed directly in stringr, which is designed to minimise the cognitive load of the user, by hiding some of the full complexity of string handling.\n(The importance of the locale argument comes up in “hidden inputs”, Chapter 5.)\nHowever, stri_opts_collator() deliberately ignores any arguments in .... This means that misspellings are silently ignored:\n\nstr_sort(x, numric = TRUE)\n#&gt; Warning in stringi::stri_opts_collator(locale, numeric = numeric, ...): Unknown\n#&gt; option to `stri_opts_collator`.\n#&gt; [1] \"x1\"   \"x100\" \"x2\"\n\nWe can work around this behaviour by adding check_dots_used() to str_sort():\n\nstr_sort &lt;- function(x, decreasing = FALSE, na_last = TRUE, locale = \"en\",  numeric = FALSE, ...) \n{\n    rlang::check_dots_used()\n  \n    stringi::stri_sort(x, \n      decreasing = decreasing, \n      na_last = na_last, \n      opts_collator = stringi::stri_opts_collator(\n        locale, \n        numeric = numeric, \n        ...\n      )\n    )\n}\n\nstr_sort(x, numric = TRUE)\n#&gt; Warning in stringi::stri_opts_collator(locale, numeric = numeric, ...): Unknown\n#&gt; option to `stri_opts_collator`.\n#&gt; Error in `str_sort()`:\n#&gt; ! Arguments in `...` must be used.\n#&gt; ✖ Problematic argument:\n#&gt; • numric = TRUE\n#&gt; ℹ Did you misspell an argument name?\n\nNote, however, that it’s better to figure out why stri_opts_collator() ignores ... in the first place. You can see that discussion at https://github.com/gagolews/stringi/issues/347.\nSee https://github.com/r-lib/devtools/issues/2016 for discussion about using this in another discussion about using this in devtools::install_github() which is an similar situation, but with a more complicated chain of calls: devtools::install_github() -&gt; install.packages() -&gt; download.file()."
  },
  {
    "objectID": "cs-mapply-pmap.html",
    "href": "cs-mapply-pmap.html",
    "title": "Case study: mapply() vs pmap()",
    "section": "",
    "text": "library(purrr)\n\nIt’s useful to compare mapply() to purrr::pmap(). They both are an attempt to solve a similar problem, extending lapply()/map() to handle iterating over any number of arguments.\n\nargs(mapply)\n#&gt; function (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) \n#&gt; NULL\nargs(pmap)\n#&gt; function (.l, .f, ..., .progress = FALSE) \n#&gt; NULL\n\n\nx &lt;- c(\"apple\", \"banana\", \"cherry\")\npattern &lt;- c(\"p\", \"n\", \"h\")\nreplacement &lt;- c(\"x\", \"f\", \"q\")\n\nmapply(gsub, pattern, replacement, x)\n#&gt;        p        n        h \n#&gt;  \"axxle\" \"bafafa\" \"cqerry\"\n\nmapply(gsub, pattern, replacement, x)\n#&gt;        p        n        h \n#&gt;  \"axxle\" \"bafafa\" \"cqerry\"\npurrr::pmap_chr(list(pattern, replacement, x), gsub)\n#&gt; [1] \"axxle\"  \"bafafa\" \"cqerry\"\n\nHere we’ll ignore simplify = TRUE which makes mapply() type-unstable by default. I’ll also ignore USE.NAMES = TRUE which isn’t just about using names, but about using character vector input as names for output. I think it’s reused from lapply() without too much thought as it’s only the names of the first argument that matter.\n\nmapply(toupper, letters[1:3])\n#&gt;   a   b   c \n#&gt; \"A\" \"B\" \"C\"\nmapply(toupper, letters[1:3], USE.NAMES = FALSE)\n#&gt; [1] \"A\" \"B\" \"C\"\nmapply(toupper, setNames(letters[1:3], c(\"X\", \"Y\", \"Z\")))\n#&gt;   X   Y   Z \n#&gt; \"A\" \"B\" \"C\"\n\npmap_chr(list(letters[1:3]), toupper)\n#&gt; [1] \"A\" \"B\" \"C\"\n\nmapply() takes the function to apply as the first argument, followed by an arbitrary number of arguments to pass to the function. This makes it different to the other apply() functions (including lapply(), sapply() and tapply()), which take the data as the first argument. mapply() could take ... as the first arguments, but that would force FUN to always be named, which would also make it inconsistent with the other apply() functions.\npmap() avoids this problem by taking a list of vectors, rather than individual vectors in .... This allows pmap() to use ... for another purpose, instead of the MoreArg argument (a list), pmap() passes ... on to .f.\n\nmapply(gsub, pattern, replacement, x, fixed = TRUE)\n#&gt;        p        n        h \n#&gt;  \"axxle\" \"bafafa\" \"cqerry\"\npurrr::pmap_chr(list(pattern, replacement, x), gsub, fixed = TRUE)\n#&gt; [1] \"axxle\"  \"bafafa\" \"cqerry\"\n\nThere’s a subtle difference here that doesn’t matter in most cases - in the mapply() fixed is recycled to the same length as pattern whereas it is not pmap(). TODO: figure out example where that’s more clear.\n(Also note that pmap() uses the . prefix to avoid the problem described in Chapter Chapter 23.)"
  },
  {
    "objectID": "cs-setNames.html#what-does-setnames-do",
    "href": "cs-setNames.html#what-does-setnames-do",
    "title": "Case study: setNames()",
    "section": "What does setNames() do?",
    "text": "What does setNames() do?\nstats::setNames() is a shorthand that allows you to set vector names inline (it’s a little surprising that it lives in the stats package). It has a simple definition:\n\nsetNames &lt;- function(object = nm, nm) {\n  names(object) &lt;- nm\n  object\n}\n\nAnd is easy to use:\n\n# Instead of\nx &lt;- 1:3\nnames(x) &lt;- c(\"a\", \"b\", \"c\")\n\n# Can write\nx &lt;- setNames(1:3, c(\"a\", \"b\", \"c\"))\nx\n#&gt; a b c \n#&gt; 1 2 3\n\nThis function is short (just two lines of code!) but yields a surprisingly rich analysis."
  },
  {
    "objectID": "cs-setNames.html#how-can-we-improve-the-names",
    "href": "cs-setNames.html#how-can-we-improve-the-names",
    "title": "Case study: setNames()",
    "section": "How can we improve the names?",
    "text": "How can we improve the names?\nFirstly, I prefer snake_case to camelCase, so I’d call the function set_names(). Then we need to consider the arguments:\n\nI think the first argument, object, would be better called x in order to emphasise that this function only works with vectors (because only vectors have names).\nThe second argument, nm is rather terse, and I don’t see any disadvantage in calling it names. I think you could also argue that it should be called y since its meaning should be obvious from the function name.\n\nThis yields:\n\nset_names &lt;- function(x = names, names) {\n  names(x) &lt;- names\n  x\n}"
  },
  {
    "objectID": "cs-setNames.html#what-about-the-default-values",
    "href": "cs-setNames.html#what-about-the-default-values",
    "title": "Case study: setNames()",
    "section": "What about the default values?",
    "text": "What about the default values?\nThe default values of setNames() are a little hard to understand, because the default value of the first argument is the second argument. It was defined this way to make it possible to name a character vector with itself:\n\nsetNames(nm = c(\"apple\", \"banana\", \"cake\"))\n#&gt;    apple   banana     cake \n#&gt;  \"apple\" \"banana\"   \"cake\"\n\nBut that decision leads to a function signature that violates one of the principles of Chapter 6: a required argument comes after an optional argument. Fortunately, we can fix this easily and still preserve the useful ability to name a vector with itself:\n\nset_names &lt;- function(x, names = x) {\n  names(x) &lt;- names\n  x\n}\n\nset_names(c(\"apple\", \"banana\", \"cake\"))\n#&gt;    apple   banana     cake \n#&gt;  \"apple\" \"banana\"   \"cake\"\n\nThis helps to emphasise that x is the primary argument."
  },
  {
    "objectID": "cs-setNames.html#what-about-bad-inputs",
    "href": "cs-setNames.html#what-about-bad-inputs",
    "title": "Case study: setNames()",
    "section": "What about bad inputs?",
    "text": "What about bad inputs?\nNow that we’ve considered how the function works with correct inputs, it’s time to consider how it should work with malformed inputs. The current function checks neither the length not the type:\n\nset_names(1:3, \"a\")\n#&gt;    a &lt;NA&gt; &lt;NA&gt; \n#&gt;    1    2    3\n\nset_names(1:3, list(letters[1:3], letters[4], letters[5:6]))\n#&gt; c(\"a\", \"b\", \"c\")                d      c(\"e\", \"f\") \n#&gt;                1                2                3\n\nWe can resolve this by asserting that the names should always be a character vector, and should have the same length as x:\n\nset_names &lt;- function(x, names = x) {\n  if (!is.character(names) || length(names) != length(x)) {\n    stop(\"`names` must be a character vector the same length as `x`.\", call. = FALSE)\n  }\n  \n  names(x) &lt;- names\n  x\n}\n\nset_names(1:3, \"a\")\n#&gt; Error: `names` must be a character vector the same length as `x`.\nset_names(1:3, list(letters[1:3], letters[4], letters[5:6]))\n#&gt; Error: `names` must be a character vector the same length as `x`.\n\nYou could also frame this test using vctrs assertions:\n\nlibrary(vctrs)\n\nset_names &lt;- function(x, names = x) {\n  vec_assert(x)\n  vec_assert(names, ptype = character(), size = length(x))\n\n  names(x) &lt;- names\n  x\n}\n\nNote that I slipped in an assertion that x should be a vector. This slightly improves the error message if you accidentally supply the wrong sort of input to set_names():\n\nsetNames(mean, 1:3)\n#&gt; Error in names(object) &lt;- nm: names() applied to a non-vector\nset_names(mean, 1:3)\n#&gt; Error in `set_names()`:\n#&gt; ! `x` must be a vector, not a function.\n\nNote that we’re simply checking the length of names here, rather than recycling it, i.e. the invariant is vec_size(set_names(x, y)) is vec_size(x), not vec_size_common(x, y). I think this is the correct behaviour because you usually add names to a vector to create a lookup table, and a lookup table is not useful if there are duplicated names. This makes set_names() less general in return for better error messages when you do something suspicious (and you can always use an explicit rep_along() if do want this behaviour.)"
  },
  {
    "objectID": "cs-setNames.html#how-could-we-extend-this-function",
    "href": "cs-setNames.html#how-could-we-extend-this-function",
    "title": "Case study: setNames()",
    "section": "How could we extend this function?",
    "text": "How could we extend this function?\nNow that we’ve modified the function so it doesn’t violate the principles in this book, we can think about how we might extend it. Currently the function is only useful for setting names to a constant. Maybe we could extend it to also make it easier to change existing names? One way to do that would be to allow names to be a function:\n\nset_names &lt;- function(x, names = x) {\n  vec_assert(x)\n  \n  if (is.function(names)) {\n    names &lt;- names(base::names(x))\n  }\n  vec_assert(names, ptype = character(), size = length(x))\n\n  names(x) &lt;- names\n  x\n}\n\nx &lt;- c(a = 1, b = 2, c = 3)\nset_names(x, toupper)\n#&gt; A B C \n#&gt; 1 2 3\n\nWe could also support anonymous function formula shortcut used in many places in the tidyverse.\n\nset_names &lt;- function(x, names = x) {\n  vec_assert(x)\n  \n  if (is.function(names) || rlang::is_formula(names)) {\n    fun &lt;- rlang::as_function(names)\n    names &lt;- fun(base::names(x))\n  }\n  vec_assert(names, ptype = character(), size = length(x))\n\n  names(x) &lt;- names\n  x\n}\n\nx &lt;- c(a = 1, b = 2, c = 3)\nset_names(x, ~ paste0(\"x-\", .))\n#&gt; x-a x-b x-c \n#&gt;   1   2   3\n\nNow set_names() supports overriding and modifying names. What about removing them? It turns out that setNames() supported this, but our stricter checks prohibit:\n\nx &lt;- c(a = 1, b = 2, c = 3)\nsetNames(x, NULL)\n#&gt; [1] 1 2 3\nset_names(x, NULL)\n#&gt; Error in `set_names()`:\n#&gt; ! `names` must be a vector, not `NULL`.\n\nWe can fix this with another clause:\n\nset_names &lt;- function(x, names = x) {\n  vec_assert(x)\n  \n  if (!is.null(names)) {\n    if (is.function(names) || rlang::is_formula(names)) {\n      fun &lt;- rlang::as_function(names)\n      names &lt;- fun(base::names(x))\n    }\n    \n  }\n\n  names(x) &lt;- names\n  x\n}\n\nx &lt;- c(a = 1, b = 2, c = 3)\nset_names(x, NULL)\n#&gt; [1] 1 2 3\n\nHowever, I think this has muddied the logic. To resolve it, I think we should pull out the checking code into a separate function. After trying out a few approaches, I ended up with:\n\ncheck_names &lt;- function(names, x) {\n  if (is.null(names)) {\n    names\n  } else if (vec_is(names)) {\n    vec_assert(names, ptype = character(), size = length(x))  \n  } else if (is.function(names)) {\n    check_names_2(names(base::names(x)), x)\n  } else if (rlang::is.formula(names)) {\n    check_names_2(rlang::as_function(names), x)\n  } else {\n    rlang::abort(\"`names` must be NULL, a function or formula, or a vector\")\n  }\n}\n\nThis then replaces vec_assert() in set_names(). I separate the input checking and implementation with a blank line to help visually group the parts of the function.\n\nset_names &lt;- function(x, names = x) {\n  vec_assert(x)\n  names &lt;- check_names(names, x)\n  \n  names(x) &lt;- names\n  x\n}\n\nWe could simplify the function even further, but I think this is a bad idea becaues it mingles input validation with implementation:\n\n# Don't do this\nset_names &lt;- function(x, names = x) {\n  vec_assert(x)\n  names(x) &lt;- check_names(names, x)\n  x\n}\n\n# Or even\nset_names &lt;- function(x, names = x) {\n  `names&lt;-`(vec_assert(x), check_names(names, x))\n}"
  },
  {
    "objectID": "cs-setNames.html#compared-to-rlangset_names",
    "href": "cs-setNames.html#compared-to-rlangset_names",
    "title": "Case study: setNames()",
    "section": "Compared to rlang::set_names()\n",
    "text": "Compared to rlang::set_names()\n\nIf you’re familiar with rlang, you might notice that we’ve ended up with something rather similar to rlang::set_names(). However, these careful analysis in this chapter has lead to a few differences. rlang::set_names():\n\nCalls the second argument nm, instead of something more descriptive. I think this is simply because we never sat down and fully considered the interface.\nCoerces nm to character vector. This allows rlang::set_names(1:4) to automatically name the vector, but this seems a relatively weak new feature in return for the cost of not throwing an error message if you provide an unsual vector type. (Both lists and data frames have as.character() methods so this will work for basically any type of vector, even if completely inappropriate.)\nPasses ... on to function nm. I now think that decision was a mistake: it substantially complicates the interface in return for a relatively small investment."
  },
  {
    "objectID": "out-multi.html#different-sizes",
    "href": "out-multi.html#different-sizes",
    "title": "Returning multiple values",
    "section": "Different sizes",
    "text": "Different sizes\nUse a list. Name it.\nIf you return the same type of output from multiple functions, you should create a function that consistently creates exact the same format (to avoid accidentally inconsistency), and consider making it an S3 class (so you can have a custom print method)."
  },
  {
    "objectID": "out-multi.html#same-size",
    "href": "out-multi.html#same-size",
    "title": "Returning multiple values",
    "section": "Same size",
    "text": "Same size\nWhen a function returns two vectors of the same size, as a general rule should you return a tibble:\n\nA matrix would only work if the vectors were the same type (and not factor or Date), doesn’t make it easy to extract the individual values, and is not easily input to other tidyverse functions.\nA list doesn’t capture the constraint that both vectors are the same length.\nA data frame is ok if you don’t want to take a dependency on tibble, but you need to remember the drawbacks: if the columns are character vectors you’ll need to remember to use stringsAsFactors = FALSE, and the print method is confusing for list- and df-cols (and you have to create by modifying an existing data frame, not by calling data.frame()). (Example: it would be weird if glue returned tibbles from a function.)"
  },
  {
    "objectID": "out-multi.html#case-study-str_locate",
    "href": "out-multi.html#case-study-str_locate",
    "title": "Returning multiple values",
    "section": "Case study: str_locate()\n",
    "text": "Case study: str_locate()\n\ne.g. str_locate(), str_locate_all()\nInteraction with str_sub()."
  },
  {
    "objectID": "out-type-stability.html#simple-examples",
    "href": "out-type-stability.html#simple-examples",
    "title": "Type-stability",
    "section": "Simple examples",
    "text": "Simple examples\n\npurrr::map() and base::lapply() are trivially type-stable because they always return lists.\n\npaste() is type stable because it always returns a character vector.\n\nvec_ptype(paste(1))\n#&gt; character(0)\nvec_ptype(paste(\"x\"))\n#&gt; character(0)\n\n\n\nbase::mean(x) almost always returns the same type of output as x. For example, the mean of a numeric vector is a numeric vector, and the mean of a date-time is a date-time.\n\nvec_ptype(mean(1))\n#&gt; numeric(0)\nvec_ptype(mean(Sys.time()))\n#&gt; POSIXct of length 0\n\n\n\nifelse() is not type-stable because the output type depends on the value:\n\nvec_ptype(ifelse(NA, 1L, 2))\n#&gt; &lt;unspecified&gt; [0]\nvec_ptype(ifelse(FALSE, 1L, 2))\n#&gt; numeric(0)\nvec_ptype(ifelse(TRUE, 1L, 2))\n#&gt; integer(0)"
  },
  {
    "objectID": "out-type-stability.html#more-complicated-examples",
    "href": "out-type-stability.html#more-complicated-examples",
    "title": "Type-stability",
    "section": "More complicated examples",
    "text": "More complicated examples\nSome functions are more complex because they take multiple input types and have to return a single output type. This includes functions like c() and ifelse(). The rules governing base R functions are idiosyncratic, and each function tends to apply it’s own slightly different set of rules. Tidy functions should use the consistent set of rules provided by the vctrs package."
  },
  {
    "objectID": "out-type-stability.html#challenge-the-median",
    "href": "out-type-stability.html#challenge-the-median",
    "title": "Type-stability",
    "section": "Challenge: the median",
    "text": "Challenge: the median\nA more challenging example is median(). The median of a vector is a value that (as evenly as possible) splits the vector into a lower half and an upper half. In the absence of ties, mean(x &gt; median(x)) == mean(x &lt;= median(x)) == 0.5. The median is straightforward to compute for odd lengths: you simply order the vector and pick the value in the middle, i.e. sort(x)[(length(x) - 1) / 2]. It’s clear that the type of the output should be the same type as x, and this algorithm can be applied to any vector that can be ordered.\nBut what if the vector has an even length? In this case, there’s no longer a unique median, and by convention we usually take the mean of the middle two numbers.\nIn R, this makes the median() not type-stable:\n\ntypeof(median(1:3))\n#&gt; [1] \"integer\"\ntypeof(median(1:4))\n#&gt; [1] \"double\"\n\nBase R doesn’t appear to follow a consistent principle when computing the median of a vector of length 2. Factors throw an error, but dates do not (even though there’s no date half way between two days that differ by an odd number of days).\n\nmedian(factor(1:2))\n#&gt; Error in median.default(factor(1:2)): need numeric data\nmedian(Sys.Date() + 0:1)\n#&gt; [1] \"2023-10-30\"\n\nTo be clear, the problems caused by this behaviour are quite small in practice, but it makes the analysis of median() more complex, and it makes it difficult to decide what principle you should adhere to when creating median methods for new vector classes.\n\nmedian(\"foo\")\n#&gt; [1] \"foo\"\nmedian(c(\"foo\", \"bar\"))\n#&gt; Warning in mean.default(sort(x, partial = half + 0L:1L)[half + 0L:1L]):\n#&gt; argument is not numeric or logical: returning NA\n#&gt; [1] NA"
  },
  {
    "objectID": "out-type-stability.html#exercises",
    "href": "out-type-stability.html#exercises",
    "title": "Type-stability",
    "section": "Exercises",
    "text": "Exercises\n\n\nHow is a date like an integer? Why is this inconsistent?\n\nvec_ptype(mean(Sys.Date()))\n#&gt; Date of length 0\nvec_ptype(mean(1L))\n#&gt; numeric(0)"
  },
  {
    "objectID": "out-vectorisation.html",
    "href": "out-vectorisation.html",
    "title": "Vectorisation",
    "section": "",
    "text": "Vectorisation has two meanings: it can refer to either the interface of a function, or its implementation. We can make a precise statement about what a vectorised interface is. A function, f, is vectorised over a vector argument, x, iff f(x)[[i]] equals f(x[[i]]), i.e. we can exchange the order of subsetting and function application. This generalises naturally to more arguments: we say f is vectorised over x and y if f(x[[i]], y[[i]]) equals f(x, y)[[i]]. A function can have some arguments that are vectorised and some that are not, f(x, ...)[[i]] equals f(x[[i]], ...).\nIt is harder to define vectorised implementation. It’s necessary for a function with a vectorised implementation to have a vectorised interface, but it also must possess the property of computational efficiency. It’s hard to make this precise, but generally it means that if there is an explicit loop, that loop is written in C or C++, not in a R."
  },
  {
    "objectID": "out-invisible.html#whats-the-pattern",
    "href": "out-invisible.html#whats-the-pattern",
    "title": "Side-effect functions should return invisibly",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nIf a function is called primarily for its side-effects, it should invisibly return a useful output. If there’s no obvious output, return the first argument. This makes it possible to use the function with in a pipeline."
  },
  {
    "objectID": "out-invisible.html#what-are-some-examples",
    "href": "out-invisible.html#what-are-some-examples",
    "title": "Side-effect functions should return invisibly",
    "section": "What are some examples?",
    "text": "What are some examples?\n\nprint(x) invisibly returns the printed object.\nx &lt;- y invisible returns y. This is what makes it possible to chain together multiple assignments x &lt;- y &lt;- z &lt;- 1\nreadr::write_csv() invisibly returns the data frame that was saved.\npurrr::walk() invisibly returns the vector iterated over.\nfs:file_copy(from, to) returns to\noptions() and par() invisibly return the previous value so you can reset with on.exit()."
  },
  {
    "objectID": "out-invisible.html#why-is-it-important",
    "href": "out-invisible.html#why-is-it-important",
    "title": "Side-effect functions should return invisibly",
    "section": "Why is it important?",
    "text": "Why is it important?\nInvisibly returning the first argument allows to call the function mid-pipe for its side-effects while allow the primary data to continue flowing through the pipe. This is useful for generating intermediate diagnostics, or for saving multiple output formats.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tibble)\n\nmtcars %&gt;%\n  as_tibble() %&gt;% \n  dplyr::filter(cyl == 6) %&gt;% \n  print() %&gt;% \n  group_by(vs) %&gt;% \n  summarise(mpg = mean(mpg))\n#&gt; # A tibble: 7 × 11\n#&gt;     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n#&gt; 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n#&gt; 3  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n#&gt; 4  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n#&gt; 5  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n#&gt; 6  17.8     6  168.   123  3.92  3.44  18.9     1     0     4     4\n#&gt; 7  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6\n#&gt; # A tibble: 2 × 2\n#&gt;      vs   mpg\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0  20.6\n#&gt; 2     1  19.1\n\n\nlibrary(readr)\n\nmtcars %&gt;% \n  write_csv(\"mtcars.csv\") %&gt;% \n  write_tsv(\"mtcars.tsv\")\n\nunlink(c(\"mtcars.csv\", \"mtcars.tsv\"))\n\n\nlibrary(fs)\n\npaths &lt;- file_temp() %&gt;%\n  dir_create() %&gt;%\n  path(letters[1:5]) %&gt;%\n  file_create()\npaths\n#&gt; /tmp/RtmpNr4zWf/file1eba798f7a80/a /tmp/RtmpNr4zWf/file1eba798f7a80/b \n#&gt; /tmp/RtmpNr4zWf/file1eba798f7a80/c /tmp/RtmpNr4zWf/file1eba798f7a80/d \n#&gt; /tmp/RtmpNr4zWf/file1eba798f7a80/e\n\nFunctions that modify some global state, like options() or par(), should return the previous value of the variables. This, in combination with compound argument pattern from Chapter 17, makes it possible to easily reset the effect of the change:\n\nx &lt;- runif(1)\nold &lt;- options(digits = 3)\nx\n#&gt; [1] 0.339\n\noptions(old)\nx\n#&gt; [1] 0.3389017"
  },
  {
    "objectID": "changes-multivers.html#whats-the-pattern",
    "href": "changes-multivers.html#whats-the-pattern",
    "title": "Work with multiple dependency versions",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nIn an ideal world, when a dependency of your package changes its interface, you want your package to work with both versions. This is more work but it has two significant advantages:\n\nThe CRAN submission process is decoupled. If your package only works with the development version of a dependency, you’ll need to carefully coordinate your CRAN submission with the dependencies CRAN submission. If your package works with both versions, you can submit first, making life easier for CRAN and for the maintainer of the dependency.\nUser code is less likely to be affected. If your package only works with the latest version of the dependency, then when a user upgrades your package, the dependency also must update. Upgrading multiple packages is more likely to affect user code than updating a single package.\n\nIn this pattern, you’ll learn how to write code designed to work with multiple versions of a dependency, and you’ll how to adapt your existing Travis configuration to test that you’ve got it right."
  },
  {
    "objectID": "changes-multivers.html#writing-code",
    "href": "changes-multivers.html#writing-code",
    "title": "Work with multiple dependency versions",
    "section": "Writing code",
    "text": "Writing code\nSometimes there will be an easy way to change the code to work with both old and new versions of the package; do this if you can! However, in most cases, you can’t, and you’ll need an if statement that runs different code for new and old versions of the package:\n\nif (dependency_has_new_interface()) {\n  # freshly written code that works with in-development dependency\n} else {\n  # existing code that works with the currently released dependency\n}\n\n(If your freshly written code uses functions that don’t exist in the CRAN version this will generate an R CMD check NOTE when you submit it to CRAN. This is one of the few NOTEs that you can explain: just mention that it’s needed for forward/backward compatibility in your submission notes.)\nWe recommend always pulling out the check out into a function so that the logic lives in one place. This will make it much easier to pull it out when it’s no longer needed, and provides a good place to document why it’s needed.\nThere are three basic approaches to implement dependency_has_new_interface():\n\nCheck the version of the package. This is recommended in most cases, but requires that the dependency author use a specific version convention.\nCheck for existence of a function.\nCheck for a specific argument value, or otherwise detect that the interface has changed.\n\nCase study: tidyr\nTo make the problem concrete so we can show of some real code, lets imagine we have a package that uses tidyr::nest(). tidyr::nest() changed substantially between 0.8.3 and 1.0.0, and so we need to write code like this:\n\nif (tidyr_new_interface()) {\n  out &lt;- tidyr::nest_legacy(df, x, y, z)\n} else {\n  out &lt;- tidyr::nest(df, c(x, y, z))\n}\n\n(As described above, when submitted to CRAN this will generate a note about missing tidyr::nest_legacy() which can be explained in the submission comments.)\nTo implement tidyr_new_interface(), we need to think about three versions of tidyr:\n\n0.8.3: the version currently on CRAN with the old interface.\n0.8.99.9000: the development version with the new interface. As usualy, the fourth component is &gt;= 9000 to indicate that it’s a development version. Note, however, that the patch version is 99; this indicates that release includes breaking changes.\n1.0.0: the future CRAN version; this is the version that will be submitted to CRAN.\n\nThe main question is how to write tidyr_new_interface(). There are three options:\n\n\nCheck that the version is greater than the development version:\n\ntidyr_new_interface &lt;- function() {\n  packageVersion(\"tidyr\") &gt; \"0.8.99\"\n}\n\nThis technique works because tidyr uses the convention that the development version of backward incompatible functions contain 99 in the third (patch) component.\n\n\nIf tidyr didn’t adopt this naming convention, we could test for the existence of unnest_legacy().\n\ntidyr_new_interface1 &lt;- function() {\n  exists(\"unnest_legacy\", asNamespace(\"tidyr\"))\n}\n\n\n\nIf the interface change was more subtle, you might have to think more creatively. If the package uses the lifecycle system, one approach would be to test for the presence of deprecated() in the function arguments:\n\ntidyr_new_interface2 &lt;- function() {\n  identical(formals(tidyr::unnest)$.drop, quote(deprecated()))\n}\n\n\n\nAll these approaches are reasonably fast, so it’s unlikely they’ll have any impact on performance unless called in a very tight loop.\n\nbench::mark(\n  version = tidyr_new_interface(),\n  exists =  tidyr_new_interface1(),\n  formals = tidyr_new_interface2() \n)[1:5]\n#&gt; # A tibble: 3 × 5\n#&gt;   expression      min   median `itr/sec` mem_alloc\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;\n#&gt; 1 version     602.1µs  700.7µs     1295.    4.01KB\n#&gt; 2 exists        3.5µs    4.6µs   177020.    3.29MB\n#&gt; 3 formals       1.7µs      2µs   363113.        0B\n\nIf you do need to use packageVersion() inside a performance sensitive function, I recommend caching the result in .onLoad() (which, by convention, lives in zzz.R). There a few ways to do this; but the following block shows one approach that matches the function interface I used above:\n\ntidyr_new_interface &lt;- function() FALSE\n.onLoad &lt;- function(...) {\n  if (utils::packageVersion(\"tidyr\") &gt; \"0.8.2\") {\n    tidyr_new_interface &lt;&lt;- function() TRUE\n  }\n}"
  },
  {
    "objectID": "changes-multivers.html#testing-with-multiple-package-versions",
    "href": "changes-multivers.html#testing-with-multiple-package-versions",
    "title": "Work with multiple dependency versions",
    "section": "Testing with multiple package versions",
    "text": "Testing with multiple package versions\nIt’s good practice to test both old and new versions of the code, but this is challenging because you can’t both sets of tests in the same R session. The easiest way to make sure that both versions are work and stay working is to use Travis.\nBefore the dependency is released, you can manually install the development version using remotes::install_github():\nmatrix:\n  include:\n  - r: release\n    name: tidyr-devel\n    before_script: Rscript -e \"remotes::install_github('tidyverse/tidyr')\"\nIt’s not generally that important to check that your code continues to work with an older version of the package, but if you want to you can use remotes::install_version():\nmatrix:\n  include:\n  - r: release\n    name: tidyr-0.8\n    before_script: Rscript -e \"remotes::install_version('tidyr', '0.8.3')\""
  },
  {
    "objectID": "changes-multivers.html#using-only-the-new-version",
    "href": "changes-multivers.html#using-only-the-new-version",
    "title": "Work with multiple dependency versions",
    "section": "Using only the new version",
    "text": "Using only the new version\nAt some point in the future, you’ll decide that the old version of the package is no longer widely used and you want to simplify your package by only depending on the new version. There are three steps:\n\nIn the DESCRIPTION, bump the required version of the dependency.\nSearch for dependency_has_new_interface(); remove the function definition and all uses (retaining the code used with the new version).\nRemove the additional build in .travis.yml."
  },
  {
    "objectID": "side-effects.html#what-is-a-side-effect",
    "href": "side-effects.html#what-is-a-side-effect",
    "title": "Side-effect soup",
    "section": "What is a side-effect?",
    "text": "What is a side-effect?\nThere are two main types of side-effect:\n\nthose that give feedback to the user.\nthose that change some global state.\n\nUser feedback\n\nSignalling a condition, with message(), warning(), or stop().\nPrinting to the console with cat().\nDrawing to the current graphics device with base graphics or grid.\nGlobal state\n\nCreating (or modifying) an existing binding with &lt;-.\nModifying the search path by attaching a package with library().\nChanging the working directory with setwd().\nModifying a file on disk with (e.g.) write.csv().\nChanging a global option with options() or a base graphics parameter with gpar().\nSetting the random seed with set.seed()\nInstalling a package.\nChanging environment variables with Sys.setenv(), or indirectly via a function like Sys.setlocale().\nModifying a variable in an enclosing environment with assign() or &lt;&lt;-.\nModifying an object with reference semantics (like R6 or data.table).\n\nMore esoteric side-effects include:\n\nDetaching a package from the search path with detach().\nChanging the library path, where R looks for packages, with .libPaths()\nChanging the active graphics device with (e.g.) png() or dev.off().\nRegistering an S4 class, method, or generic with methods::setGeneric().\nModifying the internal .Random.seed"
  },
  {
    "objectID": "side-effects.html#what-are-some-examples",
    "href": "side-effects.html#what-are-some-examples",
    "title": "Side-effect soup",
    "section": "What are some examples?",
    "text": "What are some examples?\n\n\nThe summary of a linear model includes a p-value for the overall\nregression. This value is only computed when the summary is printed: you can see it but you can’t touch it.\n\nmod &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(mod)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = mpg ~ wt, data = mtcars)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.5432 -2.3647 -0.1252  1.4096  6.8727 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\n#&gt; wt           -5.3445     0.5591  -9.559 1.29e-10 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.046 on 30 degrees of freedom\n#&gt; Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 \n#&gt; F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10"
  },
  {
    "objectID": "side-effects.html#why-is-it-bad",
    "href": "side-effects.html#why-is-it-bad",
    "title": "Side-effect soup",
    "section": "Why is it bad?",
    "text": "Why is it bad?\nSide-effect soup is bad because:\n\nIf a function does some computation and has side-effects, it can be challenging to extract the results of computation.\n\nMakes code harder to analyse because it may have non-local effects. Take this code:\n\nx &lt;- 1\ny &lt;- compute(x)\nz &lt;- calculate(x, y)\n\ndf &lt;- data.frame(x = \"x\")\n\nIf compute() or calculate() don’t have side-effects then you can predict what df will be. But if compute() did options(stringsAsFactors = FALSE) then df would now contain a character vector rather than a factor.\n\n\nSide-effect soup increases the cognitive load of a function so should be used deliberately, and you should be especially cautious when combining them with other techniques that increase cognitive load like tidy-evaluation and type-instability."
  },
  {
    "objectID": "side-effects.html#how-avoid-it",
    "href": "side-effects.html#how-avoid-it",
    "title": "Side-effect soup",
    "section": "How avoid it?",
    "text": "How avoid it?\nLocalise side-effects\nConstrain the side-effects to as small a scope as possible, and clean up automatically to avoid side-effects. withr\nExtract side-effects\nIt’s not side-effects that are bad, so much as mixing them with non-side-effect code.\nPut them in a function that is specifically focussed on the side-effect.\nIf your function is called primarily for its side-effects, it should return the primary data structure (which should be first argument), invisibly. This allows you to call it mid-pipe for its side-effects while allow the primary data to continue flowing through the pipe.\nMake side-effects noisy\nPrimary purpose of the entire package is side-effects: modifying files on disk to support package and project development. usethis functions are also designed to be noisy: as well as doing it’s job, each usethis function tells you what it’s doing.\nBut some usethis functions are building blocks for other more complex tasks.\nProvide an argument to suppress\nYou’ve probably used base::hist() for it’s side-effect of drawing a histogram:\n\nx &lt;- rnorm(1e5)\nhist(x)\n\n\n\n\nBut you might not know that hist() also returns the result of the computation. If you call plot = FALSE it will simply return the results of the computation:\n\nxhist &lt;- hist(x, plot = FALSE)\nstr(xhist)\n#&gt; List of 6\n#&gt;  $ breaks  : num [1:19] -4.5 -4 -3.5 -3 -2.5 -2 -1.5 -1 -0.5 0 ...\n#&gt;  $ counts  : int [1:18] 7 14 123 472 1638 4515 9309 15010 19170 18934 ...\n#&gt;  $ density : num [1:18] 0.00014 0.00028 0.00246 0.00944 0.03276 ...\n#&gt;  $ mids    : num [1:18] -4.25 -3.75 -3.25 -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 ...\n#&gt;  $ xname   : chr \"x\"\n#&gt;  $ equidist: logi TRUE\n#&gt;  - attr(*, \"class\")= chr \"histogram\"\n\nThis is a good approach for retro-fitting older functions while making minimal API changes. However, I think it dilutes a function to be both used for plotting and for computing so should be best avoided in newer code.\nUse the print() method\nAn alternative approach would be to always return the computation, and instead perform the output in the print() method.\nOf course ggplot2 isn’t perfect: it creates an object that specifies the plot, but there’s no easy way to extract the underlying computation so if you’ve used geom_smooth() to add lines of best fit, there’s no way to extract the values. Again, you can see the results, but you can’t touch them, which is very frustrating!\nMake easy to undo\nIf all of the above techniques fail, you should at least make the side-effect easy to undo. A use technique to do this is to make sure that the function returns the previous values, and that it can take it’s own input.\nThis is how options() and par() work. You obviously can’t eliminate those functions because their complete purpose is have global changes! But they are designed in such away that you can easily undo their operation, making it possible to apply on a local basis.\nThere are two key ideas that make these functions easy to undo:\n\n\nThey invisibly return the previous values as a list:\n\noptions(my_option = 1)\nold &lt;- options(my_option = 2)\nstr(old)\n#&gt; List of 1\n#&gt;  $ my_option: num 1\n\n\n\nInstead of n named arguments, they can take a single named list:\n\nold &lt;- options(list(my_option1 = 1, my_option2 = 2))\n\n(I wouldn’t recommend copying this technique, but I’d instead recommend always taking a single named list. This makes the function because it has a single way to call it and makes it easy to extend the API in the future, as discussed in Chapter 22)\n\n\nTogether, this means that you easily can set options temporarily.:\n\ngetOption(\"my_option1\")\n#&gt; [1] 1\n\nold &lt;- options(my_option1 = 10)\ngetOption(\"my_option1\")\n#&gt; [1] 10\noptions(old)\n\ngetOption(\"my_option1\")\n#&gt; [1] 1\n\nIf temporarily setting options in a function, you should always restore the previous values using on.exit(): this ensures that the code is run regardless of how the function exits."
  },
  {
    "objectID": "side-effects.html#package-considerations",
    "href": "side-effects.html#package-considerations",
    "title": "Side-effect soup",
    "section": "Package considerations",
    "text": "Package considerations\nCode in package is executed at build-time.i.e. if you have:\n\nx &lt;- Sys.time()\n\nFor mac and windows, this will record when CRAN built the binary. For linux, when the package was installed.\nBeware copying functions from other packages:\n\nfoofy &lt;- barfy::foofy\n\nVersion of barfy might be different between run-time and build-time.\nIntroduces a build-time dependency.\nhttps://github.com/r-lib/devtools/issues/1788"
  },
  {
    "objectID": "spooky-action.html#whats-the-problem",
    "href": "spooky-action.html#whats-the-problem",
    "title": "Spooky action",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nThere are no limits to what an function or script can do. After you call draw_plot() or source(\"analyse-data.R\"), you could discover that all the variables in your global environment have been deleted, or that 1000 new files have been created on your desktop. But these actions would be surprising, because generally you expect the impact of a function (or script) to be as limited as possible. Collectively, we call such side-effects “spooky actions” because the connection between action (calling a function or sourcing a script) and result (deleting objects or upgrading packages) is surprising. It’s like flipping a light-switch and discovering that the shower starts running, or having a poltergeist that rearranges the contents of your kitchen cupboards when you’re not looking.\nDeleting variables and creating files on your desktop are obviously surprising even if you’ve only just started using R. But there are other actions that are less obviously destructive, and only start to become surprising as your mental model of R matures. These include actions like:\n\nAttaching packages with library(). For example, ggplot2::geom_map() used to call library(maps) in order to make map data available to the function. This seems harmless, but if you were using purrr, it would break map() map() would now refer to maps::map() rather than purrr::map(). Because of functions in different packages can have the same name, attaching a package can change the behaviour of existing code.\nInstalling packages with install.packages(). If a script needs dplyr to work, and it’s not installed, it seems polite to install it on behalf of the user. But installing a new package can upgrade existing packages, which might break code in other projects. Install a package is a potentially destructive operation which should be done with care.\nDeleting objects in the global environment with rm(list = ls()). This might seem like a good way to reset the environment so that your script can run cleanly. But if someone else source()s your script, it will delete objects that might be important to them. (Of course, you’d hope that all of those objects could easily be recreated from another script, but that is not always the case).\n\nBecause R doesn’t constrain the potential scope of functions and scripts, you have to. By avoiding these actions, you will create code that is less surprising to other R users. At first, this might seem like tedious busywork. You might find that spooky action is convenient in the moment, and you might convince yourself that it’s necessary or a good idea. But as you share your code with more people and run more code that has been shared with you1, you’ll find spooky action to get more and more surprising and frustrating."
  },
  {
    "objectID": "spooky-action.html#what-precisely-is-a-spooky-action",
    "href": "spooky-action.html#what-precisely-is-a-spooky-action",
    "title": "Spooky action",
    "section": "What precisely is a spooky action?",
    "text": "What precisely is a spooky action?\nWe can make the notion of spooky action precise by thinking about trees. Code should only affect the tree beneath where it lives, so any action that reaches up, or across, the tree is a spooky action.\nThere are two important types of trees to consider:\n\n\nThe tree formed by files and directories. A script should only read from and write to directories beneath the directory where it lives. This explains why you shouldn’t install packages (because the package library usually lives elsewhere), and also explains why you shouldn’t create files on the desktop.\nThis rule can be relaxed in two small ways. Firstly, if the script lives in a project, it’s ok to read from and write to anywhere in the project (i.e. a file in R/ can read from data-raw/ and write to data/). Secondly, it’s always ok to write to the session specific temporary directory, tempdir(). This directory is automatically deleted when R closes, so does not have any lasting effects.\n\nThe tree of environments created by function calls. A function should only create and modify variables in its own environment or environments that it creates (typically by calling other functions). This explains why you shouldn’t attach packages (because that changes the search path), why you shouldn’t delete variables with rm(list = ls()), or assign to variables that you didn’t create with &lt;&lt;-."
  },
  {
    "objectID": "spooky-action.html#how-can-i-remediate-spooky-actions",
    "href": "spooky-action.html#how-can-i-remediate-spooky-actions",
    "title": "Spooky action",
    "section": "How can I remediate spooky actions?",
    "text": "How can I remediate spooky actions?\nIf you have read the above cautions, and still want to proceed, there are three ways you can make the spooky action as safe as possible:\n\nAllow the user to control the scope.\nMake the action less spooky by giving it a name that clearly describes what it will do.\nExplicitly check with the user before proceeding with the action.\nAdvertise what’s happening, so while the action might still be spooky, at least it isn’t surprising.\n\nParameterise the action\nThe first technique is to allow the user to control where the action will occur. For example, instead of save_output_desktop(), you would write save_output(path), and require that the user provide the path.\nAdvertise the action with a clear name\nIf you can’t parameterise the action, make it clear what’s going to happen from the outside. It is fine for function or scripts to have actions outside of their usual trees as long as it is implicit in the name:\n\nIt’s ok for &lt;- to modify the global environment, because that is its one job, and it’s obvious from the name (once you’ve learned about &lt;-, which happens very early). Similarly, it’s ok for save_output_to_desktop() to create files in on the desktop, or copy_to_clipboard() to copy text to the clipboard, because the action is clear from the name.\nIt’s fine for install.packages() to modify files outside of the current working directory because it’s designed specifically to install packages. Similarly, it’s ok for source(\"class-setup.R\") to install packages because the intent of a setup script is to get your computer into the same state as someone else’s.\n\nHere, it’s the name of the function or script that is really important. As soon as you\nNote that it’s the name that’s important - it’s fine for install.packages() to install packages, but it’s not ok as soon as it’s hidden behind even a very simple wrapper:\n\ncurrent_time &lt;- function() {\n  if (!requireNamespace(\"lubridate\", quietly = TRUE)) {\n    install.packages(\"lubridate\")\n  }\n  lubridate::now()\n}\ncurrent_time()\n#&gt; [1] \"2023-10-30 15:06:57 UTC\"\n\nAsk for confirmation\nIf you can’t parameterise the operation, and need to perform it from somewhere deep within the cope, make sure to confirm with the user before performing the action. The code below shows how you might do so when installing a package:\n\ninstall_if_needed &lt;- function(package) {\n  if (requireNamespace(package, quietly = TRUE)) {\n    return(invisible(TRUE))\n  }\n  \n  if (!interactive()) {\n    stop(package, \" is not installed\", call. = FALSE)\n  }\n  \n  title &lt;- paste0(package, \" is not installed. Do you wish to install now?\")\n  if (menu(c(\"Yes\", \"No\"), title = title) != 1) {\n    stop(\"Confirmation not received\", call. = FALSE)\n  }\n  \n  invisible(TRUE)\n}\n\nNote the use of interactive() here: if the user is not in an interactive setting (i.e. the code is being run with Rscript) and we can not get explicit confirmation, we shouldn’t make any changes. Also that all failures are errors: this ensures that the remainder of the function or script does not run if the user doesn’t confirm.\nIdeally this function would also clearly describe the consequences of your decision. For example, it would be nice to know if it will download a significant amount of data (since you might want to wait until your at a fast connection if downloading a 1 Gb data package), or if it will upgrade existing packages (since that might break other code).\nWriting code that checks with the user requires some care, and it’s easy to get the details wrong. That’s why it’s better to prefer one of the prior techniques.\nAdvertise the side-effects\nIf you can’t get explicit confirmation from the user, at the very minimum you should clearly advertise what is happening. For example, when you call install.packages() it notifies you:\n\ninstall.packages(\"dplyr\")\n#&gt; Installing package into ‘/Users/hadley/R’\n#&gt; (as ‘lib’ is unspecified)\n#&gt; Trying URL 'https://cran.rstudio.com/bin/macosx/el-capitan/contrib/3.5/dplyr_0.8.0.1.tgz'\n#&gt; Content type 'application/x-gzip' length 6587031 bytes (6.3 MB)\n#&gt; ==================================================\n#&gt; downloaded 6.3 MB\n\nHowever, this message could do with some work:\n\nIt says installing “package”, without specifying which package (so if this is called inside another function it won’t be informative).\nIt doesn’t notify me which dependencies it’s also going to update.\nIt notifies me of the url it’s downloading from, which I don’t care about, and it only notifies me about the size when it’s finished downloading, by which time it too late to stop it.\n\nI would instead write something like this:\n\ninstall.packages(\"dplyr\")\n#&gt; Installing package dplyr to `/Users/hadley/R`\n#&gt; Also installing 3 dependencies: glue, rlang, R6\n\nWe’ll come back to the issue of informing the user in …"
  },
  {
    "objectID": "spooky-action.html#case-studies",
    "href": "spooky-action.html#case-studies",
    "title": "Spooky action",
    "section": "Case studies",
    "text": "Case studies\n\nsave() and load()\n\nload() has a spooky action because it modifies variables in the current environment:\n\nx &lt;- 1\nload(\"spooky-action.rds\")\nx\n#&gt; [1] 10\n\nYou can make it less spooky by supplying verbose = TRUE. Here we learn that it also loaded a y object:\n\nload(\"spooky-action.rds\", verbose = TRUE)\n#&gt; Loading objects:\n#&gt;   x\n#&gt;   y\ny\n#&gt; [1] 100\n\n(In an ideal world verbose = TRUE would be default)\nBut generally, I’d avoid save() and load() altogether, and instead use saveRDS() and readRDS(), which read and write individual R objects to individual R files and work with &lt;-. This eliminates all spooky action:\n\nsaveRDS(x, \"x.rds\")\nx &lt;- readRDS(\"x.rds\")\n\n\nunlink(\"x.rds\")\n\n(readr provides readr::read_rds() and readr::write_rds() if the inconsistent naming conventions bother you like they bother me.)\nusethis\nThe usethis package is designed to support the process of developing a package of R code. It automates many of tedious setup steps by providing function like use_r() or use_test(). Many usethis functions modify the DESCRIPTION and create other files. usethis makes these actions as pedestrian as possible by:\n\nMaking it clear that the entire package is designed for the purpose of creating and modifying files, and the purpose of each function is clearly encoded in its named.\n\nFor any potential risky operation, e.g. overwriting an existing file, usethis explicitly asks for confirmation from the user. To make it harder to “click” through prompts without reading them, usethis uses random prompts in a random ordering.\n\nusethis::ui_yeah(\"Do you want to proceed?\")\n#&gt; Do you want to proceed?\n#&gt; \n#&gt; 1: Absolutely not\n#&gt; 2: Not now\n#&gt; 3: I agree\n\nusethis also works in concert with git to make sure that change are captured in a way that can easily be undone.\n\n\nWhen you call it, every usethis function describes what it is doing as it it doing it:\n\nusethis::create_package(\"mypackage\", open = FALSE)\n#&gt; ✔ Creating 'mypackage'\n#&gt; ✔ Setting active project to 'mypackage'\n#&gt; ✔ Creating 'R/'\n#&gt; ✔ Writing 'DESCRIPTION'\n#&gt; ✔ Writing 'NAMESPACE'\n#&gt; ✔ Writing 'mypackage.Rproj'\n#&gt; ✔ Adding '.Rproj.user' to '.gitignore'\n#&gt; ✔ Adding '^mypackage\\\\.Rproj$', '^\\\\.Rproj\\\\.user$' to '.Rbuildignore'\n\nThis is important, but it’s not clear how impactful it is because many functions produce enough output that reading through it all seems onerous and so generally most people don’t read it.\n\n&lt;&lt;-\nIf you haven’t heard of &lt;&lt;-, the super-assignment operator, before, feel free to skip this section as it’s an advanced technique that has relatively limited applications. They’re most important in the context of functionals, which you can read more about in Advanced R.\n&lt;&lt;- is safe if you use it to modify a variable in an environment that you control. For example, the following code creates a function that counts the number of times it is called. The use of &lt;&lt;- is safe because it only affects the environment created by make_counter(), not an external environment.\n\nmake_counter &lt;- function() {\n  i &lt;- 0\n  function() {\n    i &lt;&lt;- i + 1\n    i\n  }\n}\nc1 &lt;- make_counter()\nc2 &lt;- make_counter()\nc1()\n#&gt; [1] 1\nc1()\n#&gt; [1] 2\nc2()\n#&gt; [1] 1\n\nA more common use of &lt;&lt;- is to break one of the limitations of map()2 and use it like a for loop to iteratively modify input. For example, imagine you want to compute a cumulative sum. That’s straightforward to write with a for loop:\n\nx &lt;- rpois(10, 10)\n\nout &lt;- numeric(length(x))\nfor (i in seq_along(x)) {\n  if (i == 1) {\n    out[[i]] &lt;- x[[i]]\n  } else {\n    out[[i]] &lt;- x[[i]] + out[[i - 1]]\n  }\n}\n\nrbind(x, out)\n#&gt;     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt; x     12    8    8   15   14    9    9    8    8    14\n#&gt; out   12   20   28   43   57   66   75   83   91   105\n\nA simple transformation from to use map() doesn’t work:\n\nlibrary(purrr)\nout &lt;- numeric(length(x))\nmap_dbl(seq_along(x), function(i) {\n  if (i == 1) {\n    out[[i]] &lt;- x[[i]]\n  } else {\n    out[[i]] &lt;- x[[i]] + out[[i - 1]]\n  }\n})\n#&gt;  [1] 12  8  8 15 14  9  9  8  8 14\n\nBecause the modification of out is happening inside of a function, R creates a copy of out (this is called copy-on-modify principle). Instead we need to use &lt;&lt;- to reach outside of the function to modify the outer out:\n\nmap_dbl(seq_along(x), function(i) {\n  if (i == 1) {\n    out[[i]] &lt;&lt;- x[[i]]\n  } else {\n    out[[i]] &lt;&lt;- x[[i]] + out[[i - 1]]\n  }\n})\n#&gt;  [1]  12  20  28  43  57  66  75  83  91 105\n\nThis use of &lt;&lt;- is a spooky action because we’re reaching up the tree of environments to modify an object created outside of the function. In this case, however, there’s no point in using map(): the point of those functions is to restrict what you can do compared to a for loop so that your code is easier to understand. R for data science has other examples of for loops that could be rewritten with map(), but shouldn’t be.\nNote that we could still wrap this code into a function to eliminate the spooky action:\n\ncumsum2 &lt;- function(x) {\n  out &lt;- numeric(length(x))\n  map_dbl(seq_along(x), function(i) {\n    if (i == 1) {\n      out[[i]] &lt;&lt;- x[[i]]\n    } else {\n      out[[i]] &lt;&lt;- x[[i]] + out[[i - 1]]\n    }\n  })\n  out\n}\n\nThis eliminates the spooky action because it’s now modifying an object that the function “owns”, but I still wouldn’t recommend it, as the use of map() and &lt;&lt;- only increases complexity for no gain compared to the use of a for loop.\n\nassign() in a for loop\nIt’s not uncommon for people to ask how to create multiple objects from for a loop. For example, maybe you have a vector of file names, and want to read each file into an individual object. With some effort you typically discover assign():\n\npaths &lt;- c(\"a.csv\", \"b.csv\", \"c.csv\")\nnames(paths) &lt;- c(\"a\", \"b\", \"c\")\n\nfor (i in seq_along(paths)) {\n  assign(names(paths)[[i]], read.csv(paths[[i]]))\n}\n\nThe main problem with this approach is that it does not facilitate composition. For example, imagine that you now wanted to figure out how many rows are in each of the data frames. Now you need to learn how to loop over a series of objects, where the name of the object is stored in a character vector. With some work, you might discover get() and write:\n\nlengths &lt;- numeric(length(names))\nfor (i in seq_along(paths)) {\n  lengths[[i]] &lt;- nrow(get(names(paths)[[i]]))\n}\n\nThis approach is not necessarily bad in and of itself3, but it tends to lead you down a high-friction path. Instead, if you learn a little about lists and functional programming techniques (e.g. with purrr), you’ll be able to write code like this:\n\nlibrary(purrr)\n\nfiles &lt;- map(paths, read.csv)\nlengths &lt;- map_int(files, nrow)\n\nThis obviously requires that you learn some new tools - but learning about map() and map_int() will pay off in many more situations than learning about assign() and get(). And because you can reuse map() and friends in many places, you’ll find that they get easier and easier to use over time.\nIt would certainly be possible to build tools in purrr to avoid having to learn about assign() and get() and to provide a polished interface for working with character vectors containing object names. But such functions would need to reach up the tree of environments, so would violate the “spooky action” principle, and thus I believe are best avoided."
  },
  {
    "objectID": "spooky-action.html#footnotes",
    "href": "spooky-action.html#footnotes",
    "title": "Spooky action",
    "section": "",
    "text": "Spooky actions tend to be particularly frustrating to those who teach R, because they have to run scripts from many students, and those scripts can end up doing wacky things to their computers.↩︎\npurrr::map() is basically interchangeable with base::lapply() so if you’re more familiar with lapply(), you can mentally substitute it for map() in all the code here.↩︎\nHowever, if you take this approach in multiple places in your code, you’ll need to make sure that you don’t use the same name in multiple loops because assign() will silently overwrite an existing variable. This might not happen commonly but because it’s silent, it will create a bug that is hard to detect.↩︎"
  },
  {
    "objectID": "err-call.html",
    "href": "err-call.html",
    "title": "Error call",
    "section": "",
    "text": "Don’t display the call when generating an error message. Either use stop(call. = FALSE) or rlang::abort() to avoid it.\nWhy not? Typically doesn’t display enough information to find the source of the call (since most errors are not from top-level function calls), and you can expect the most people to either use RStudio, or know how to call traceback()."
  },
  {
    "objectID": "err-constructor.html#whats-the-pattern",
    "href": "err-constructor.html#whats-the-pattern",
    "title": "Error constructors",
    "section": "What’s the pattern?",
    "text": "What’s the pattern?\nFollowing the rule of three, whenever you generate the same error in three or more places, you should extract it out into a common function, called an error constructor. This function should create a custom condition that contains components that can easily be tested and a conditionMessage() method that generates user friendly error messages.\n(This is a new pattern that we are currently rolling out across the tidyverse; it’s currently found in few packages.)\n\nlibrary(rlang)"
  },
  {
    "objectID": "err-constructor.html#why-is-this-important",
    "href": "err-constructor.html#why-is-this-important",
    "title": "Error constructors",
    "section": "Why is this important?",
    "text": "Why is this important?\n\nIf you don’t use an custom condition, you can only check that your function has generated the correct error by matching the text of the error message with a regular expression. This is fragile because the text of error messages changes relatively frequently, causing spurious test failures.\nYou can use custom conditions for one-off errors, but generally the extra implementation work is not worth the pay off. That’s why we recommend only using an error constructor for repeated errors.\nIt gives more precise control over error handling with tryCatch(). This is particularly useful in packages because you may be able to give more useful high-level error mesasges by wrapping a specific low-level error.\nAs you start using this technique for more error messages you can create a hierarchy of errors that allows you to borrow behaviour, reducing the amount of code you need to write.\nOnce you have identified all the errors that can be thrown by a function, you can add a @section Throws: to the documentation that precisely describes the possible failure modes."
  },
  {
    "objectID": "err-constructor.html#what-does-an-error-constructor-do",
    "href": "err-constructor.html#what-does-an-error-constructor-do",
    "title": "Error constructors",
    "section": "What does an error constructor do?",
    "text": "What does an error constructor do?\nAn error constructor is very similar to an S3 constructor, as its job is to extract out repeated code and generate a rich object that can easily be computed with. The primary difference is that instead of creating and returning a new object, it creates a custom error and immediately throws it with abort().\nHere’s a simple imaginary error that might be thrown by fs if it couldn’t find a file:\n\nstop_not_found &lt;- function(path) {\n  abort(\n    .subclass = \"fs_error_not_found\",\n    path = path\n  )\n}\n\nNote the naming scheme:\n\nThe function should be called stop_{error_type}\nThe error class should be {package}_error_{error_type}.\n\nThe function should have one argument for each varying part of the error, and these argument should be passed onto abort() to be stored in the condition object.\nTo generate the error message shown to the user, provide a conditionMessage() method:\n\n#' @export\nconditionMessage.fs_error_not_found &lt;- function(c) {\n  glue::glue_data(c, \"'{path}' not found\")\n}\n\n\nstop_not_found(\"a.csv\")\n#&gt; Error: 'a.csv' not found\n\nThis method must be exported, because you are defining a method for a generic in another package, and it will often use glue::glue_data() to assemble the components of the condition into a string. See https://style.tidyverse.org/error-messages.html for advice on writing the error message."
  },
  {
    "objectID": "err-constructor.html#how-do-i-test",
    "href": "err-constructor.html#how-do-i-test",
    "title": "Error constructors",
    "section": "How do I test?",
    "text": "How do I test?\n\nlibrary(testthat)\n#&gt; \n#&gt; Attaching package: 'testthat'\n#&gt; The following objects are masked from 'package:rlang':\n#&gt; \n#&gt;     is_false, is_null, is_true\n\nTest the constructor\nFirstly, you should test the error constructor. The primary goal of this test is to ensure that the error constructor generates a message that is useful to humans, which you can not automate. This means that you can not use a unit test (because the desired output is not known) and instead you need to use a regression test, so you can ensure that the message does not change unexpectedly. For that reason the best approach is usually to use verify_output(), e.g.:\n\ntest_that(\"stop_not_found() generates useful error message\", {\n  verify_output(test_path(\"test-stop-not-found.txt\"), {\n    stop_not_found(\"a.csv\")\n  })\n})\n\nThis is useful for pull requests because verify_output() generates a complete error messages in a text file that can easily be read and reviewed.\nIf your error has multiple arguments, or your conditionMessage() method contains if statements, you should generally attempt to cover them all in a test case.\nTest usage\nNow that you have an error constructor, you’ll need to slightly change how you test your functions that use the error constructor. For example, take this imaginary example for reading a file into a single string:\n\nread_lines &lt;- function(x) {\n  if (!file.exists(x)) {\n    stop_not_found(x)\n  }\n  paste0(readLines(x), collapse = \"\\n\")\n}\n\nPreviously, you might have written:\n\nexpect_error(read_lines(\"missing-file.txt\"), \"not found\")\n\nBut, now as you see, testthat gives you a warning that suggests you need to use the class argument instead:\n\nexpect_error(read_lines(\"missing-file.txt\"), class = \"fs_error_not_found\")\n\nThis is less fragile because you can now change the error message without having to worry about breaking existing tests.\nIf you also want to check components of the error object, note that expect_error() returns it:\n\ncnd &lt;- expect_error(read_lines(\"missing-file.txt\"), class = \"fs_error_not_found\")\nexpect_equal(cnd$path, \"missing-file.txt\")\n\nI don’t think this level of testing is generally important, so you should only use it because the error generation code is complex conditions, or you have identified a bug."
  },
  {
    "objectID": "err-constructor.html#error-hierarchies",
    "href": "err-constructor.html#error-hierarchies",
    "title": "Error constructors",
    "section": "Error hierarchies",
    "text": "Error hierarchies\nAs you start writing more and more error constructors, you may notice that you are starting to share code between them because the errors form a natural hierarchy. To take advantage of this hierarchy to reduce the amount of code you need to write, you can make the errors subclassable by adding ... and class arguments:\n\nstop_not_found &lt;- function(path, ..., class = character()) {\n  abort(\n    .subclass = c(class, \"fs_error_not_found\"),\n    path = path\n  )\n}\n\nThen the subclasses can call this constructor, and the problem becomes one of S3 class design. We currently have little experience with this, so use with caution."
  }
]